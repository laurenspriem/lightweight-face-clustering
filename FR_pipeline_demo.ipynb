{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "import cv2\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf;\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed up the notebook, set this value to True to skip intermediate demo steps and only run full pipeline at the end of this notebook\n",
    "skip_intermediate_demo = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "LFW subset locally downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing the subset of LFW dataset\n",
    "# dataset = 'lfw_10classes_20images'\n",
    "dataset = 'lfw_30classes_20images'\n",
    "# dataset = 'lfw_30classes_2images'\n",
    "# dataset = 'lfw_10images'\n",
    "# dataset = 'lfw_original' # crashes the kernel, try in Paperspace!\n",
    "data_path = path + '/data/' + dataset + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the subset data into a NumPy array\n",
    "data = []\n",
    "original_images = []\n",
    "targets = []\n",
    "targets = []\n",
    "for subdir in os.listdir(data_path):\n",
    "    subdir_path = os.path.join(data_path, subdir)\n",
    "    if os.path.isdir(subdir_path):\n",
    "        for file in os.listdir(subdir_path):\n",
    "            file_path = os.path.join(subdir_path, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                img = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
    "                RGB_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                data.append(RGB_img.flatten())\n",
    "                original_images.append(RGB_img)\n",
    "                targets.append(subdir)\n",
    "\n",
    "data = np.array(data)\n",
    "original_images = np.array(original_images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n, w, h, c = original_images.shape\n",
    "\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "target_names = targets\n",
    "target_names_unique = [target_name for target_name in set(target_names)]\n",
    "y = LabelEncoder().fit_transform(target_names.copy())\n",
    "target_names_first_index = [np.where(y == label)[0][0] for label in np.unique(y)]\n",
    "target_names_first_index.sort()\n",
    "\n",
    "n_classes = len(target_names_unique)\n",
    "\n",
    "subdir_counts = {subdir: targets.count(subdir) for subdir in set(targets)}\n",
    "n_classes_larger_than_5 = len([count for count in subdir_counts.values() if count >= 5])\n",
    "n_classes_larger_than_10 = len([count for count in subdir_counts.values() if count >= 10])\n",
    "n_images_from_classes_smaller_than_5 = sum([count for count in subdir_counts.values() if count < 5])\n",
    "n_images_from_classes_smaller_than_10 = sum([count for count in subdir_counts.values() if count < 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the data array\n",
    "print('Data shape:', data.shape)\n",
    "\n",
    "# Print the list of subdirectories and the number of images per subdirectory\n",
    "print('Subdirectory counts (unordered):', subdir_counts,'\\n')\n",
    "\n",
    "print(f\"There are {n} images in the dataset, {13233 - n} less than the original dataset (of 13233).\")\n",
    "print(f\"Each image is {w} pixels wide and {h} pixels tall, with {c} channels (RGB).\")\n",
    "print(f\"In total this gives us {n_features} features per image.\")\n",
    "print(f\"There are {n_classes} unique targets (classes) in the dataset, of which {n_classes_larger_than_5} have at least five images, and {n_classes_larger_than_10} have at least ten images.\")\n",
    "print(f\"There are {n_images_from_classes_smaller_than_5} images from classes with less than five images, and {n_images_from_classes_smaller_than_10} images from classes with less than ten images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images = original_images[target_names_first_index[0:10]]\n",
    "label_array = np.array(target_names)[target_names_first_index[0:10]]\n",
    "\n",
    "# Create a figure with a 2x5 grid of subplots\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "\n",
    "# Iterate over the subplots and plot each image with its corresponding label\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(plot_images[i])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(str(label_array[i]))\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the channels: RGB\n",
    "original_images[0,0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(original_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(original_images[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_original_faces(indices, max_faces=None, start=0):\n",
    "    # Get the relevant images from the original_images array\n",
    "    relevant_images = original_images[indices[start:]]\n",
    "    \n",
    "    # If max_faces is set, limit the number of images to plot\n",
    "    if max_faces is not None:\n",
    "        relevant_images = relevant_images[:max_faces]\n",
    "    \n",
    "    # Determine the number of rows and columns to use in the plot\n",
    "    num_images = relevant_images.shape[0]\n",
    "    num_cols = min(num_images, 5)\n",
    "    num_rows = int(np.ceil(num_images / num_cols))\n",
    "    \n",
    "    # Create the plot and add subplots for each image\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 15))\n",
    "    axs = axs.flatten()\n",
    "    for i, img in enumerate(relevant_images):\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].axis('off')\n",
    "    \n",
    "    # Remove any unused subplots\n",
    "    for j in range(i+1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face detection\n",
    "Based on BlazeFace, taken inspiration from [this repo](https://github.com/ibaiGorordo/BlazeFace-TFLite-Inference). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from src.blazeface import BlazeFace, BlazeFaceResults\n",
    "# reload(src.BlazeFace.BlazeFace)\n",
    "# reload(BlazeFaceResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blazeface_model = BlazeFace(model_type='sparse')\n",
    "blazeface_model.input_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_test_image = original_images[0]\n",
    "test_image = blazeface_model.preprocess_input(original_test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_image.numpy().squeeze()) if test_image.shape[0] == 1 else plt.imshow(test_image[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_results = blazeface_model.detect_faces(original_test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_results.boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_results.keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_results.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(original_test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_image = blazeface_model.draw_detections_single_image(original_test_image, face_detection_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(plot_test_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_intermediate_demo:\n",
    "    blazeface_dense_model = BlazeFace(model_type='dense', score_threshold=0.6)\n",
    "\n",
    "    face_detection_results = []\n",
    "    detected_faces_count = 0\n",
    "    detected_faces_indices = []\n",
    "    undetected_faces_count = 0\n",
    "    undetected_faces_indices = []\n",
    "    double_detected_faces_count = 0\n",
    "    double_detected_faces_indices = []\n",
    "\n",
    "    for i in tqdm(range(original_images.shape[0])):\n",
    "        face_detection_results.append(blazeface_dense_model.detect_faces(original_images[i]))\n",
    "        if face_detection_results[-1].boxes.shape[0] > 0:\n",
    "            detected_faces_count += 1\n",
    "            detected_faces_indices.append(i)\n",
    "            if face_detection_results[-1].boxes.shape[0] > 1:\n",
    "                double_detected_faces_count += 1\n",
    "                double_detected_faces_indices.append(i)\n",
    "\n",
    "        else:\n",
    "            undetected_faces_count += 1\n",
    "            undetected_faces_indices.append(i)\n",
    "\n",
    "    print(f\"For the dense model:\")\n",
    "    print(f\"Detected faces: {detected_faces_count} out of {original_images.shape[0]} ({detected_faces_count/original_images.shape[0]*100:.2f}%). Undetected faces: {undetected_faces_count} out of {original_images.shape[0]} ({undetected_faces_count/original_images.shape[0]*100:.2f}%).\")\n",
    "    print(f\"Double detected faces: {double_detected_faces_count} out of {original_images.shape[0]} ({double_detected_faces_count/original_images.shape[0]*100:.2f}%).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_intermediate_demo:\n",
    "    blazeface_model = BlazeFace(model_type='sparse', score_threshold=0.6)\n",
    "\n",
    "    face_detection_results = []\n",
    "    detected_faces_count = 0\n",
    "    detected_faces_indices = []\n",
    "    undetected_faces_count = 0\n",
    "    undetected_faces_indices = []\n",
    "    double_detected_faces_count = 0\n",
    "    double_detected_faces_indices = []\n",
    "\n",
    "    for i in tqdm(range(original_images.shape[0])):\n",
    "        face_detection_results.append(blazeface_model.detect_faces(original_images[i]))\n",
    "        if face_detection_results[-1].boxes.shape[0] > 0:\n",
    "            detected_faces_count += 1\n",
    "            detected_faces_indices.append(i)\n",
    "            if face_detection_results[-1].boxes.shape[0] > 1:\n",
    "                double_detected_faces_count += 1\n",
    "                double_detected_faces_indices.append(i)\n",
    "\n",
    "        else:\n",
    "            undetected_faces_count += 1\n",
    "            undetected_faces_indices.append(i)\n",
    "\n",
    "    print(f\"Detected faces: {detected_faces_count} out of {original_images.shape[0]} ({detected_faces_count/original_images.shape[0]*100:.2f}%). Undetected faces: {undetected_faces_count} out of {original_images.shape[0]} ({undetected_faces_count/original_images.shape[0]*100:.2f}%).\")\n",
    "    print(f\"Double detected faces: {double_detected_faces_count} out of {original_images.shape[0]} ({double_detected_faces_count/original_images.shape[0]*100:.2f}%).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_face_detections(indices, title, images=original_images, labels=target_names):\n",
    "    \n",
    "    plot_images = images[indices]\n",
    "    label_array = np.array(labels)[indices]\n",
    "    \n",
    "    num_images = plot_images.shape[0]\n",
    "\n",
    "    for i in range(num_images):\n",
    "        detection_result = blazeface_model.detect_faces(plot_images[i])\n",
    "        plot_images[i] = blazeface_model.draw_detections_single_image(plot_images[i], detection_result)\n",
    "\n",
    "    # Calculate the number of rows and columns for the grid\n",
    "    cols = min(5, num_images)\n",
    "    rows = int(np.ceil(num_images / cols))\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(3*cols, 3*rows))\n",
    "\n",
    "    # Customize the background color and text color\n",
    "    bg_color = \"#222222\"\n",
    "    text_color = \"#ffffff\"\n",
    "\n",
    "    fig.patch.set_facecolor(bg_color)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_images:\n",
    "            # Plot the image\n",
    "            if plot_images.shape[-1] == 3:\n",
    "                ax.imshow(plot_images[i].astype(np.uint8))\n",
    "            else:\n",
    "                ax.imshow(plot_images[i], cmap='gray')\n",
    "            \n",
    "            # Set the title with the corresponding label\n",
    "            ax.set_title(f\"{label_array[i]}\", fontsize=12, color=text_color, y=1.02)\n",
    "        \n",
    "        # Remove axis ticks\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        # Set the background color for each subplot\n",
    "        ax.set_facecolor(bg_color)\n",
    "\n",
    "        # Set the color of the spines (border of the subplots)\n",
    "        ax.spines['bottom'].set_color(text_color)\n",
    "        ax.spines['top'].set_color(text_color)\n",
    "        ax.spines['right'].set_color(text_color)\n",
    "        ax.spines['left'].set_color(text_color)\n",
    "\n",
    "    fig.suptitle(title, fontsize=20, y=1.02 + 0.02 * rows, color=text_color)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 1 - 0.04 * rows])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_intermediate_demo:\n",
    "    plot_face_detections(target_names_first_index, \"Detected faces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_intermediate_demo:\n",
    "    plot_face_detections(undetected_faces_indices, \"Undetected faces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_intermediate_demo:\n",
    "    plot_face_detections(double_detected_faces_indices, \"Double face detections\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face alignment\n",
    "\n",
    "For now I'm simply following [this github](https://sefiks.com/2020/02/23/face-alignment-for-face-recognition-in-python-within-opencv/) script from insightface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.face_alignment import SimpleFaceAlignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = original_images[18]\n",
    "test_image_face_detection_result = blazeface_model.detect_faces(test_image)\n",
    "plt.imshow(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_aligner = SimpleFaceAlignment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_aligned_image_results = face_aligner.crop_and_align(test_image, test_image_face_detection_result)\n",
    "test_aligned_image_results.aligned_faces[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_aligned_image_results.aligned_faces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_aligned_image_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_intermediate_demo:\n",
    "    face_detection_results = []\n",
    "    face_alignment_results = []\n",
    "\n",
    "    for idx in tqdm(range(original_images.shape[0])):\n",
    "        image = original_images[idx]\n",
    "        face_detection_results.append(blazeface_model.detect_faces(image))\n",
    "        if (face_detection_results[-1].boxes.shape[0] > 0) & (face_detection_results[-1].boxes.shape[0] < 2):\n",
    "            face_alignment_results.append(face_aligner.crop_and_align(image, face_detection_results[idx]))\n",
    "        else:\n",
    "            face_alignment_results.append(None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aligned_faces(aligned_face_results_list, max_faces=None, indices=None, start=0):\n",
    "    if indices is not None:\n",
    "        aligned_face_results_list = [aligned_face_results_list[i] for i in indices]\n",
    "\n",
    "    aligned_faces = [\n",
    "        face\n",
    "        for aligned_face_results in aligned_face_results_list\n",
    "        if aligned_face_results is not None\n",
    "        for face in aligned_face_results.aligned_faces\n",
    "    ]\n",
    "\n",
    "    if start > 0:\n",
    "        aligned_faces = aligned_faces[start:]\n",
    "\n",
    "    if max_faces is not None:\n",
    "        aligned_faces = aligned_faces[:max_faces]\n",
    "\n",
    "    num_faces = len(aligned_faces)\n",
    "    cols = min(5, num_faces)\n",
    "    rows = int(np.ceil(num_faces / cols))\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "    fig.suptitle(\"Aligned Faces\", fontsize=20, y=1.02)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_faces:\n",
    "            if aligned_faces[i].shape[-1] == 3:\n",
    "                ax.imshow(aligned_faces[i].astype(np.uint8))\n",
    "            else:\n",
    "                ax.imshow(aligned_faces[i], cmap='gray')\n",
    "            ax.set_title(f\"Face {i + 1 + start}\")\n",
    "\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_intermediate_demo:\n",
    "    plot_aligned_faces(face_alignment_results, max_faces=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_intermediate_demo:\n",
    "    plot_aligned_faces(face_alignment_results, max_faces=50, indices=target_names_first_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face transformation\n",
    "\n",
    "Follows [this](https://github.com/deepinsight/insightface/blob/be3f7b3e0e635b56d903d845640b048247c41c90/common/face_align.py) example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.face_alignment import ArcFaceAlignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = original_images[25]\n",
    "plt.imshow(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_face_detection_result = blazeface_model.detect_faces(test_image)\n",
    "if not skip_intermediate_demo:\n",
    "    test_image_face_detection_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_landmarks_relative_full = test_image_face_detection_result.keypoints[0][:4]\n",
    "test_image_landmarks_absolute_full = (test_image_landmarks_relative_full * 250).astype(np.int32)\n",
    "if not skip_intermediate_demo:\n",
    "    test_image_landmarks_relative_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_face = test_image.copy()\n",
    "test_image_face = blazeface_model.draw_detections_single_image(test_image_face, test_image_face_detection_result)\n",
    "plt.imshow(test_image_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_face = SimpleFaceAlignment()\n",
    "test_image_cropped, xmin, ymin = crop_face.crop(test_image, test_image_face_detection_result, 250,250)\n",
    "test_image_cropped = test_image_cropped[0][:112,:112]\n",
    "plt.imshow(test_image_cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcface_align = ArcFaceAlignment()\n",
    "test_image_align_result = arcface_align.crop_and_align(test_image, test_image_face_detection_result)\n",
    "test_image_align_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_image_align_result.aligned_faces[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_intermediate_demo:\n",
    "    face_detection_results = []\n",
    "    face_transformation_results = []\n",
    "\n",
    "    for idx in tqdm(range(original_images.shape[0])):\n",
    "        image = original_images[idx]\n",
    "        face_detection_results.append(blazeface_model.detect_faces(image))\n",
    "        if (face_detection_results[-1].boxes.shape[0] > 0) & (face_detection_results[-1].boxes.shape[0] < 2):\n",
    "            face_transformation_results.append(arcface_align.crop_and_align(image, face_detection_results[idx]))\n",
    "        else:\n",
    "            face_transformation_results.append(None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_intermediate_demo:\n",
    "    plot_aligned_faces(face_transformation_results, max_faces=30, start=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_intermediate_demo:\n",
    "    plot_aligned_faces(face_alignment_results, max_faces=30, start=40)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face normalization (optional)\n",
    "\n",
    "I can use a face mesh model like the one from [MediaPipe](https://github.com/google/mediapipe/blob/master/docs/solutions/face_mesh.md) to normalize faces, as shown [here](https://sefiks.com/2022/05/29/normalization-for-facial-recognition-with-mediapipe/). This probably gives better results, since the bounding boxes and landmarks from BlazeFace are not super accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.face_alignment import FaceMeshAlignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_mesh_model = FaceMeshAlignment(crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = original_images[18]\n",
    "test_image_face_detection_result = blazeface_model.detect_faces(test_image)\n",
    "test_image_aligned_faces = face_aligner.crop_and_align(test_image, test_image_face_detection_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_image_aligned_faces.aligned_faces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_normalized_faces = face_mesh_model.normalize_faces(test_image, test_image_face_detection_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_image_normalized_faces.aligned_faces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run face detection and alignment on all images again\n",
    "if not skip_intermediate_demo:\n",
    "    face_detection_results = []\n",
    "    face_normalization_results = []\n",
    "\n",
    "    for idx in tqdm(range(original_images.shape[0])):\n",
    "        image = original_images[idx]\n",
    "        face_detection_results.append(blazeface_model.detect_faces(image))\n",
    "        if (face_detection_results[-1].boxes.shape[0] > 0) & (face_detection_results[-1].boxes.shape[0] < 2):\n",
    "            face_normalization_results.append(face_mesh_model.normalize_faces(image, face_detection_results[idx]))\n",
    "        else:\n",
    "            face_normalization_results.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_intermediate_demo:\n",
    "    plot_aligned_faces(face_normalization_results, max_faces=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_intermediate_demo:\n",
    "    plot_aligned_faces(face_alignment_results, max_faces=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_intermediate_demo:\n",
    "    plot_aligned_faces(face_normalization_results, max_faces=20, start=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_intermediate_demo:\n",
    "    plot_aligned_faces(face_alignment_results, max_faces=20, start=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_intermediate_demo:\n",
    "    plot_aligned_faces(face_normalization_results, max_faces=100, start=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mobilefacenet import MobileFaceNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilefacenet_model = MobileFaceNet(model_type=\"ente_web\")\n",
    "# mobilefacenet_quantized_model = MobileFaceNet(model_type=\"quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilefacenet_model.interpreter.get_input_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilefacenet_model.interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run face detection and alignment/normalization on all images again\n",
    "face_detection_results = []\n",
    "face_alignment_results = []\n",
    "# face_normalization_results = []\n",
    "\n",
    "for idx in tqdm(range(original_images.shape[0])):\n",
    "    image = original_images[idx]\n",
    "    face_detection_results.append(blazeface_model.detect_faces(image))\n",
    "    if face_detection_results[-1].boxes.shape[0] > 0:\n",
    "        face_alignment_results.append(arcface_align.crop_and_align(image, face_detection_results[idx]))\n",
    "        # face_normalization_results.append(face_mesh_model.normalize_faces(image, face_detection_results[idx]))\n",
    "    else:\n",
    "        face_alignment_results.append(None)\n",
    "        # face_normalization_results.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create face embeddings using unquantized MobileFaceNet model\n",
    "face_embeddings = []\n",
    "# face_embeddings_normalized = []\n",
    "\n",
    "for idx in tqdm(range(original_images.shape[0])):\n",
    "    if (face_alignment_results[idx] is not None): # if face was detected\n",
    "        if (face_alignment_results[idx].num_faces ==1): # if only one face was detected\n",
    "            face_embeddings.append(mobilefacenet_model.extract_embedding(face_alignment_results[idx].aligned_faces[0]))\n",
    "        else:\n",
    "            face_embeddings.append(None)\n",
    "    else:\n",
    "        face_embeddings.append(None)\n",
    "    # if (face_normalization_results[idx] is not None): # if face was detected\n",
    "    #     if (face_normalization_results[idx].num_faces ==1): # if only one face was detected\n",
    "    #         face_embeddings_normalized.append(mobilefacenet_model.extract_embedding(face_normalization_results[idx].aligned_faces[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(face_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_embeddings_squeezed = [array[0] if array is not None else None for array in face_embeddings]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing the embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick approach with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_embeddings_used = face_embeddings\n",
    "\n",
    "# filtered_embeddings = [(idx, arr[0]) for idx, arr in enumerate(face_embeddings) if arr is not None]\n",
    "filtered_embeddings = [(idx, arr[0]) for idx, arr in enumerate(face_embeddings_used) if arr is not None]\n",
    "\n",
    "# Separate indices and data into two lists\n",
    "original_indices, filtered_data_arrays = zip(*filtered_embeddings)\n",
    "minimum_cluster_size_general = 5\n",
    "true_labels = y[np.array(original_indices)]\n",
    "label_counts = Counter(true_labels)\n",
    "label_map = {label: label if count >= minimum_cluster_size_general else -1 for label, count in label_counts.items()}\n",
    "true_labels_with_noise = np.vectorize(label_map.get)(true_labels)\n",
    "# Create a pandas DataFrame from the data arrays\n",
    "df_embeddings = pd.DataFrame(filtered_data_arrays)\n",
    "\n",
    "# Add the original index column\n",
    "df_embeddings['original_index'] = original_indices\n",
    "\n",
    "# Calculate the cosine distance matrix\n",
    "temp = time.time()\n",
    "cosine_dist_matrix = cosine_distances(df_embeddings.drop(columns=['original_index'])).astype(np.float64)\n",
    "cosine_distance_time = time.time() - temp\n",
    "temp = time.time()\n",
    "cosine_dist_matrix_more_precise = pairwise_distances(df_embeddings.drop(columns=['original_index']), metric='cosine').astype(np.float64)\n",
    "cosine_pairwise_distance_time = time.time() - temp\n",
    "\n",
    "norm_data = normalize(df_embeddings.drop(columns=['original_index']), norm='l2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(true_labels == 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solid approach with sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform HDBSCAN clustering\n",
    "\n",
    "# # Try out values for the parameters\n",
    "# minimum_cluster_size = 5\n",
    "# minimum_samples = 1\n",
    "# selection_epsilon = 0\n",
    "# selection_method = 'eom'\n",
    "\n",
    "# # My best values for the parameters\n",
    "# minimum_cluster_size = 7\n",
    "# minimum_samples = 1\n",
    "# selection_epsilon = 0\n",
    "# selection_method = 'eom'\n",
    "\n",
    "# Default values for the parameters\n",
    "minimum_cluster_size = 5\n",
    "minimum_samples = 5\n",
    "selection_epsilon = 0\n",
    "selection_method = 'eom'\n",
    "\n",
    "# # Shailesh's best values for the parameters\n",
    "# minimum_cluster_size = 3\n",
    "# minimum_samples = 5\n",
    "# selection_epsilon = 0.6\n",
    "# selection_method = 'leaf'\n",
    "\n",
    "# # Standard euclidian distance metric\n",
    "# clusterer_euclidian = HDBSCAN()\n",
    "# cluster_labels_euclidian = clusterer_euclidian.fit_predict(df_embeddings.drop(columns=['original_index']))\n",
    "\n",
    "\n",
    "# Cosine distance metric (precomputed via distance matrix (INEFFICIENT, but best accuracy)))\n",
    "temp = time.time()\n",
    "clusterer_precomp = HDBSCAN(metric='precomputed', min_cluster_size=minimum_cluster_size, min_samples=minimum_samples, cluster_selection_epsilon=selection_epsilon, cluster_selection_method=selection_method)\n",
    "cluster_labels_cosine = clusterer_precomp.fit_predict(cosine_dist_matrix_more_precise)\n",
    "hdbscan_cosine_time = time.time() - temp\n",
    "hdbscan_cosine_time += min(cosine_distance_time, cosine_pairwise_distance_time)\n",
    "\n",
    "# # Angular distance (similar to cosine) via euclidian with normalized data (possibly best balance of accuracy and memory efficiency)\n",
    "temp = time.time()\n",
    "clusterer_euclidian = HDBSCAN(min_cluster_size=minimum_cluster_size, min_samples=minimum_samples, cluster_selection_epsilon=selection_epsilon, cluster_selection_method=selection_method) # euclidean distance is the default\n",
    "cluster_labels_angular = clusterer_euclidian.fit_predict(norm_data)\n",
    "hdbscan_angular_time = time.time() - temp\n",
    "\n",
    "# Add the cluster labels as a new column in the DataFrame\n",
    "df_embeddings['cluster_label'] = cluster_labels_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_clusters_cosine = len(np.unique(cluster_labels_cosine))-1\n",
    "ari_cosine = adjusted_rand_score(true_labels_with_noise, cluster_labels_cosine)\n",
    "ari_cosine_exnoise = adjusted_rand_score(true_labels_with_noise[cluster_labels_cosine != -1], cluster_labels_cosine[cluster_labels_cosine != -1])\n",
    "ami_cosine = adjusted_mutual_info_score(true_labels_with_noise, cluster_labels_cosine)\n",
    "ami_cosine_exnoise = adjusted_mutual_info_score(true_labels_with_noise[cluster_labels_cosine != -1], cluster_labels_cosine[cluster_labels_cosine != -1])\n",
    "percentage_noise_cosine = np.sum(cluster_labels_cosine==-1)/len(cluster_labels_cosine)*100\n",
    "\n",
    "number_clusters_angular = len(np.unique(cluster_labels_angular))-1\n",
    "ari_angular = adjusted_rand_score(true_labels_with_noise, cluster_labels_angular)\n",
    "ari_angular_exnoise = adjusted_rand_score(true_labels_with_noise[cluster_labels_angular != -1], cluster_labels_angular[cluster_labels_angular != -1])\n",
    "ami_angular = adjusted_mutual_info_score(true_labels_with_noise, cluster_labels_angular)\n",
    "ami_angular_exnoise = adjusted_mutual_info_score(true_labels_with_noise[cluster_labels_angular != -1], cluster_labels_angular[cluster_labels_angular != -1])\n",
    "percentage_noise_angular = np.sum(cluster_labels_angular==-1)/len(cluster_labels_angular)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Results for the cosine distance metric:\\n\")\n",
    "\n",
    "print(f\"Number of different clusters: {number_clusters_cosine}\")\n",
    "print(f\"ARI score for clustering accuracy: {ari_cosine}\")\n",
    "print(f\"ARI score excluding noise: {ari_cosine_exnoise}\")\n",
    "print(f\"AMI score for clustering accuracy: {ami_cosine}\")\n",
    "print(f\"AMI score excluding noise: {ami_cosine_exnoise}\")\n",
    "print(f\"Percentage of faces clustered into noise: {percentage_noise_cosine:.2f}%\")\n",
    "print(f\"Time needed for clustering: {hdbscan_cosine_time:.2f}s for cosine\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To easily interpret ARI:\n",
    "The Adjusted Rand Index (ARI) measures the similarity between two clusterings, with 1 indicating a perfect match and 0 implying agreement no better than random chance. A higher ARI value signifies greater similarity between the two clusterings, while a negative value indicates the agreement is worse than random chance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the reason for the low accuracy and high amount of noise is the fact that HDBSCAN works less well on high-dimensional data. The embeddings are in 192 dimensions, while the documentation of HDBSCAN recommends usage for up to 50 a 100 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Results for the angular distance metric:\\n\")\n",
    "\n",
    "print(f\"Number of different clusters: {number_clusters_angular}\")\n",
    "print(f\"ARI score for clustering accuracy: {ari_angular}\")\n",
    "print(f\"ARI score excluding noise: {ari_angular_exnoise}\")\n",
    "print(f\"AMI score for clustering accuracy: {ami_angular}\")\n",
    "print(f\"AMI score excluding noise: {ami_angular_exnoise}\")\n",
    "print(f\"Percentage of faces clustered into noise: {percentage_noise_angular:.2f}%\")\n",
    "print(f\"Time needed for clustering: {hdbscan_angular_time:.2f}s for angular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_clusters(distance_matrix, labels, figsize=(10, 8), n_components=2, metric='precomputed', init='random', random_state=42):\n",
    "    # Perform t-SNE on the embeddings\n",
    "    tsne = TSNE(n_components=n_components, metric=metric, init=init, random_state=random_state)\n",
    "    embeddings_2d = tsne.fit_transform(distance_matrix)\n",
    "\n",
    "    # Add the 2D embeddings to a DataFrame\n",
    "    df_embeddings = pd.DataFrame({'x': embeddings_2d[:, 0], 'y': embeddings_2d[:, 1], 'cluster_label': labels})\n",
    "\n",
    "    # Define the colormap\n",
    "    n_clusters = len(np.unique(labels))\n",
    "    if n_clusters <= 10:\n",
    "        tab10_cmap = plt.get_cmap('tab10', np.unique(labels).size - 1)\n",
    "        tab10_cmap_list = tab10_cmap(np.linspace(0, 1, np.unique(labels).size - 1))\n",
    "        custom_cmap_list = np.vstack(([0.7, 0.7, 0.7, 1.], tab10_cmap_list))\n",
    "        cmap = mcolors.LinearSegmentedColormap.from_list('custom_cmap', custom_cmap_list)\n",
    "    else:\n",
    "        cmap = plt.get_cmap('nipy_spectral', np.unique(labels).size - 1)\n",
    "\n",
    "    # Plot the clusters\n",
    "    plt.figure(figsize=figsize)\n",
    "    scatter = plt.scatter(df_embeddings['x'], df_embeddings['y'], c=df_embeddings['cluster_label'], cmap=cmap, s=50)\n",
    "    plt.title(\"t-SNE visualization of clusters\")\n",
    "\n",
    "    # Add a legend for the cluster colors\n",
    "    if n_clusters <= 10:\n",
    "        labels = [\"noise\"] + [f\"Cluster {i}\" for i in range(1, n_clusters)]\n",
    "        handles, _ = scatter.legend_elements()\n",
    "        legend1 = plt.legend(handles, labels, loc=\"upper left\", title=\"Clusters\")\n",
    "        plt.gca().add_artist(legend1)\n",
    "    else:\n",
    "        # Only show legend for noise\n",
    "        noise_proxy = plt.scatter([], [], c='grey', s=50)\n",
    "        plt.legend([noise_proxy], [\"noise\"], loc=\"upper left\", title=\"Clusters\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_clusters(cosine_dist_matrix, df_embeddings['cluster_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_noise = list(df_embeddings[df_embeddings['cluster_label'] == -1][\"original_index\"].values)\n",
    "print(f\"Number of faces clustered into noise: {len(hdbscan_noise)} (out of {n_images_from_classes_smaller_than_5}/{n_images_from_classes_smaller_than_10} for min_samples=5/10)\")\n",
    "plot_original_faces(hdbscan_noise, max_faces=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform DBSCAN clustering\n",
    "\n",
    "# test parameters\n",
    "dbscan_epsilon = 0.3\n",
    "dbscan_min_samples = 5 \n",
    "\n",
    "# default parameters\n",
    "# dbscan_epsilon = 0.5 # 0.5 is default\n",
    "# dbscan_min_samples = 5 # 5 is default (integer values only)\n",
    "\n",
    "temp = time.time()\n",
    "dbscan = DBSCAN(eps=dbscan_epsilon, min_samples=dbscan_min_samples, metric='precomputed')\n",
    "cluster_labels_dbscan = dbscan.fit_predict(cosine_dist_matrix)\n",
    "dbscan_time = time.time() - temp\n",
    "\n",
    "# Add the cluster labels as a new column in the DataFrame\n",
    "df_embeddings['dbscan_cluster_label'] = cluster_labels_dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_clusters_dbscan = len(np.unique(cluster_labels_dbscan))-1\n",
    "ari_dbscan = adjusted_rand_score(true_labels_with_noise, cluster_labels_dbscan)\n",
    "ari_dbscan_exnoise = adjusted_rand_score(true_labels_with_noise[cluster_labels_dbscan != -1], cluster_labels_dbscan[cluster_labels_dbscan != -1])\n",
    "ami_dbscan = adjusted_mutual_info_score(true_labels_with_noise, cluster_labels_dbscan)\n",
    "ami_dbscan_exnoise = adjusted_mutual_info_score(true_labels_with_noise[cluster_labels_dbscan != -1], cluster_labels_dbscan[cluster_labels_dbscan != -1])\n",
    "percentage_noise_dbscan = np.sum(cluster_labels_dbscan==-1)/len(cluster_labels_dbscan)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Results for the DBSCAN clustering:\\n\")\n",
    "\n",
    "print(f\"Number of different clusters: {number_clusters_dbscan}\")\n",
    "print(f\"ARI score for clustering accuracy: {ari_dbscan:.4f}\")\n",
    "print(f\"ARI score excluding noise: {ari_dbscan_exnoise:.4f}\")\n",
    "print(f\"AMI score for clustering accuracy: {ami_dbscan:.4f}\")\n",
    "print(f\"AMI score excluding noise: {ami_dbscan_exnoise:.4f}\")\n",
    "print(f\"Percentage of faces clustered into noise: {percentage_noise_dbscan:.2f}%\")\n",
    "print(f\"Time taken for DBSCAN clustering: {dbscan_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_clusters(cosine_dist_matrix, df_embeddings['dbscan_cluster_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_noise = list(df_embeddings[df_embeddings['dbscan_cluster_label'] == -1][\"original_index\"].values)\n",
    "print(f\"Number of faces clustered into noise: {len(dbscan_noise)} (out of {n_images_from_classes_smaller_than_5}/{n_images_from_classes_smaller_than_10} for min_samples=5/10)\")\n",
    "plot_original_faces(dbscan_noise, max_faces=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform OPTICS (xi-steep method) clustering\n",
    "\n",
    "# default parameters\n",
    "minimum_samples_optics = 2 # 5 is default (integer values only)\n",
    "minimum_cluster_size_optics = 7 # 0.05 is default (int)\n",
    "maximum_eps_optics = np.inf # np.inf is default (float or None)\n",
    "optics_method = 'xi' # 'xi' is default ('xi' or 'dbscan')\n",
    "optics_dbscan_eps = 0.3 # 0.5 is default (float)\n",
    "optics_xi = 0.05 # 0.05 is default (float)\n",
    "\n",
    "temp = time.time()\n",
    "optics = OPTICS(min_samples = minimum_samples_optics, max_eps= maximum_eps_optics, metric='cosine', cluster_method=optics_method, eps=optics_dbscan_eps, xi=optics_xi, min_cluster_size=minimum_cluster_size_optics)\n",
    "cluster_labels_optics = optics.fit_predict(norm_data)\n",
    "optics_time = time.time() - temp\n",
    "\n",
    "# Add the cluster labels as a new column in the DataFrame\n",
    "df_embeddings['optics_cluster_label'] = cluster_labels_optics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_clusters_optics = len(np.unique(cluster_labels_optics))-1\n",
    "ari_optics = adjusted_rand_score(true_labels_with_noise, cluster_labels_optics)\n",
    "ari_optics_exnoise = adjusted_rand_score(true_labels_with_noise[cluster_labels_optics != -1], cluster_labels_optics[cluster_labels_optics != -1])\n",
    "ami_optics = adjusted_mutual_info_score(true_labels_with_noise, cluster_labels_optics)\n",
    "ami_optics_exnoise = adjusted_mutual_info_score(true_labels_with_noise[cluster_labels_optics != -1], cluster_labels_optics[cluster_labels_optics != -1])\n",
    "percentage_noise_optics = np.sum(cluster_labels_optics==-1)/len(cluster_labels_optics)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Results for the OPTICS clustering:\\n\")\n",
    "\n",
    "print(f\"Number of different clusters: {number_clusters_optics}\")\n",
    "print(f\"ARI score for clustering accuracy: {ari_optics:.4f}\")\n",
    "print(f\"ARI score excluding noise: {ari_optics_exnoise:.4f}\")\n",
    "print(f\"AMI score for clustering accuracy: {ami_optics:.4f}\")\n",
    "print(f\"AMI score excluding noise: {ami_optics_exnoise:.4f}\")\n",
    "print(f\"Percentage of faces clustered into noise: {percentage_noise_optics:.2f}%\")\n",
    "print(f\"Time taken for OPTICS clustering: {optics_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_clusters(cosine_dist_matrix, df_embeddings['optics_cluster_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optics_noise = list(df_embeddings[df_embeddings['optics_cluster_label'] == -1][\"original_index\"].values)\n",
    "print(f\"Number of faces clustered into noise: {len(optics_noise)} (out of {n_images_from_classes_smaller_than_5}/{n_images_from_classes_smaller_than_10} for min_samples=5/10)\")\n",
    "plot_original_faces(optics_noise, max_faces=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN vs DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '-'*80 + '\\n')\n",
    "print(\"DATA INFORMATION:\\n\")\n",
    "# Print the shape of the data array\n",
    "print(f\"Dataset: '{dataset}'\")\n",
    "print('Data shape:', data.shape)\n",
    "\n",
    "# Print the list of subdirectories and the number of images per subdirectory\n",
    "print('Subdirectory counts (unordered):', subdir_counts,'\\n')\n",
    "\n",
    "print(f\"There are {n} images in the dataset, {13233 - n} less than the original dataset (of 13233).\")\n",
    "print(f\"Each image is {w} pixels wide and {h} pixels tall, with {c} channels (RGB).\")\n",
    "print(f\"In total this gives us {n_features} features per image.\")\n",
    "print(f\"There are {n_classes} unique targets (classes) in the dataset, of which {n_classes_larger_than_5} have at least five images, and {n_classes_larger_than_10} have at least ten images.\")\n",
    "print(f\"There are {n_images_from_classes_smaller_than_5} images from classes with less than five images, and {n_images_from_classes_smaller_than_10} images from classes with less than ten images.\")\n",
    "\n",
    "\n",
    "print('\\n' + '-'*80 + '\\n')\n",
    "\n",
    "\n",
    "print(\"CLUSTERING LATENCY:\\n\")\n",
    "# print the latency of each clustering algorithm\n",
    "print(\"Time taken for each algorithm:\")\n",
    "print(f\"HDBSCAN (cosine): {hdbscan_cosine_time:.2f}s\")\n",
    "print(f\"HDBSCAN (angular): {hdbscan_angular_time:.2f}s\")\n",
    "print(f\"DBSCAN: {dbscan_time:.2f}s\")\n",
    "# print(f\"OPTICS: {optics_time:.2f}s\")\n",
    "\n",
    "\n",
    "print('\\n' + '-'*80 + '\\n')\n",
    "\n",
    "\n",
    "print(\"CLUSTERING ACCURACY:\\n\")\n",
    "\n",
    "print(\"Scores for each algorithm:\\n\")\n",
    "\n",
    "print(\"ARI scores:\")\n",
    "print(f\"HDBSCAN (cosine): {ari_cosine:.4f}\")\n",
    "print(f\"HDBSCAN (angular): {ari_angular:.4f}\")\n",
    "print(f\"DBSCAN: {ari_dbscan:.4f}\\n\")\n",
    "\n",
    "print(\"Percentage of faces clustered into noise:\")\n",
    "print(f\"HDBSCAN (cosine): {percentage_noise_cosine:.2f}%\")\n",
    "print(f\"HDBSCAN (angular): {percentage_noise_angular:.2f}%\")\n",
    "print(f\"DBSCAN: {percentage_noise_dbscan:.2f}%\\n\")\n",
    "\n",
    "print(\"ARI scores excluding noise:\")\n",
    "print(f\"HDBSCAN (cosine): {ari_cosine_exnoise:.4f}\")\n",
    "print(f\"HDBSCAN (angular): {ari_angular_exnoise:.4f}\")\n",
    "print(f\"DBSCAN: {ari_dbscan_exnoise:.4f}\\n\")\n",
    "\n",
    "print(\"AMI scores:\")\n",
    "print(f\"HDBSCAN (cosine): {ami_cosine:.4f}\")\n",
    "print(f\"HDBSCAN (angular): {ami_angular:.4f}\")\n",
    "print(f\"DBSCAN: {ami_dbscan:.4f}\\n\")\n",
    "\n",
    "print(\"Percentage of faces clustered into noise:\")\n",
    "print(f\"HDBSCAN (cosine): {percentage_noise_cosine:.2f}%\")\n",
    "print(f\"HDBSCAN (angular): {percentage_noise_angular:.2f}%\")\n",
    "print(f\"DBSCAN: {percentage_noise_dbscan:.2f}%\\n\")\n",
    "\n",
    "print(\"AMI scores excluding noise:\")\n",
    "print(f\"HDBSCAN (cosine): {ami_cosine_exnoise:.4f}\")\n",
    "print(f\"HDBSCAN (angular): {ami_angular_exnoise:.4f}\")\n",
    "print(f\"DBSCAN: {ami_dbscan_exnoise:.4f}\\n\")\n",
    "\n",
    "print(f\"Number of different clusters: ({n_classes_larger_than_5}/{n_classes_larger_than_10} for min_samples=5/10)\")\n",
    "print(f\"HDBSCAN (cosine): {number_clusters_cosine}\")\n",
    "print(f\"HDBSCAN (angular): {number_clusters_angular}\")\n",
    "print(f\"DBSCAN: {number_clusters_dbscan}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fa9909cf6832301c6b2b44f12b07c926ee1a1dd9a700423e1c9e9cf2469e7b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
