<!DOCTYPE html>
<!-- saved from url=(0054)https://google.github.io/mediapipe/solutions/face_mesh -->
<html lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">  <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="./Face Mesh _ mediapipe_files/just-the-docs-default.css"> <script async="" src="./Face Mesh _ mediapipe_files/js"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-140696581-2'); </script> <script src="./Face Mesh _ mediapipe_files/lunr.min.js"></script> <script src="./Face Mesh _ mediapipe_files/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Face Mesh | mediapipe</title> <meta name="generator" content="Jekyll v3.9.3"> <meta property="og:title" content="Face Mesh"> <meta property="og:locale" content="en_US"> <meta name="description" content="Cross-platform, customizable ML solutions for live and streaming media."> <meta property="og:description" content="Cross-platform, customizable ML solutions for live and streaming media."> <link rel="canonical" href="https://google.github.io/mediapipe/solutions/face_mesh.html"> <meta property="og:url" content="https://google.github.io/mediapipe/solutions/face_mesh.html"> <meta property="og:site_name" content="mediapipe"> <meta property="og:type" content="website"> <meta name="twitter:card" content="summary"> <meta property="twitter:title" content="Face Mesh"> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Cross-platform, customizable ML solutions for live and streaming media.","headline":"Face Mesh","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://google.github.io/mediapipe/images/logo_horizontal_color.png"}},"url":"https://google.github.io/mediapipe/solutions/face_mesh.html"}</script> <!-- End Jekyll SEO tag --> <style data-emotion="css"></style></head> <body> <a class="skip-to-main" href="https://google.github.io/mediapipe/solutions/face_mesh#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"></path> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"></path> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"></path> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"></path> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header"> <a href="https://google.github.io/mediapipe/" class="site-title lh-tight"> <div class="site-logo"></div> </a> <a href="https://google.github.io/mediapipe/solutions/face_mesh#" id="menu-button" class="site-button"> <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg> </a> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="https://google.github.io/mediapipe/" class="nav-list-link">Home</a></li><li class="nav-list-item"><a href="https://google.github.io/mediapipe/solutions/face_mesh#" class="nav-list-expander" aria-label="toggle links in Getting Started category"> <svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg> </a><a href="https://google.github.io/mediapipe/getting_started/getting_started.html" class="nav-list-link">Getting Started</a><ul class="nav-list"><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/face_mesh#" class="nav-list-expander" aria-label="toggle links in MediaPipe on Android category"> <svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg> </a><a href="https://google.github.io/mediapipe/getting_started/android.html" class="nav-list-link">MediaPipe on Android</a><ul class="nav-list"><li class="nav-list-item "> <a href="https://google.github.io/mediapipe/getting_started/hello_world_android.html" class="nav-list-link">Hello World! on Android</a> </li><li class="nav-list-item "> <a href="https://google.github.io/mediapipe/getting_started/android_solutions.html" class="nav-list-link">MediaPipe Android Solutions</a> </li><li class="nav-list-item "> <a href="https://google.github.io/mediapipe/getting_started/android_archive_library.html" class="nav-list-link">MediaPipe Android Archive</a> </li></ul></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/face_mesh#" class="nav-list-expander" aria-label="toggle links in MediaPipe on iOS category"> <svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg> </a><a href="https://google.github.io/mediapipe/getting_started/ios.html" class="nav-list-link">MediaPipe on iOS</a><ul class="nav-list"><li class="nav-list-item "> <a href="https://google.github.io/mediapipe/getting_started/hello_world_ios.html" class="nav-list-link">Hello World! on iOS</a> </li></ul></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/face_mesh#" class="nav-list-expander" aria-label="toggle links in MediaPipe in Python category"> <svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg> </a><a href="https://google.github.io/mediapipe/getting_started/python.html" class="nav-list-link">MediaPipe in Python</a><ul class="nav-list"><li class="nav-list-item "> <a href="https://google.github.io/mediapipe/getting_started/python_framework.html" class="nav-list-link">MediaPipe Python Framework</a> </li></ul></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/getting_started/javascript.html" class="nav-list-link">MediaPipe in JavaScript</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/face_mesh#" class="nav-list-expander" aria-label="toggle links in MediaPipe in C++ category"> <svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg> </a><a href="https://google.github.io/mediapipe/getting_started/cpp.html" class="nav-list-link">MediaPipe in C++</a><ul class="nav-list"><li class="nav-list-item "> <a href="https://google.github.io/mediapipe/getting_started/hello_world_cpp.html" class="nav-list-link">Hello World! in C++</a> </li></ul></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/getting_started/install.html" class="nav-list-link">Installation</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/getting_started/gpu_support.html" class="nav-list-link">GPU Support</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/getting_started/help.html" class="nav-list-link">Getting Help</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/getting_started/faq.html" class="nav-list-link">FAQ</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/getting_started/troubleshooting.html" class="nav-list-link">Troubleshooting</a></li></ul></li><li class="nav-list-item active"><a href="https://google.github.io/mediapipe/solutions/face_mesh#" class="nav-list-expander" aria-label="toggle links in Solutions category"> <svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg> </a><a href="https://google.github.io/mediapipe/solutions/solutions.html" class="nav-list-link">Solutions</a><ul class="nav-list"><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/face_detection.html" class="nav-list-link">Face Detection</a></li><li class="nav-list-item active"><a href="https://google.github.io/mediapipe/solutions/face_mesh.html" class="nav-list-link active">Face Mesh</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/iris.html" class="nav-list-link">Iris</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/hands.html" class="nav-list-link">Hands</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/face_mesh#" class="nav-list-expander" aria-label="toggle links in Pose category"> <svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg> </a><a href="https://google.github.io/mediapipe/solutions/pose.html" class="nav-list-link">Pose</a><ul class="nav-list"><li class="nav-list-item "> <a href="https://google.github.io/mediapipe/solutions/pose_classification.html" class="nav-list-link">Pose Classification</a> </li></ul></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/holistic.html" class="nav-list-link">Holistic</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/selfie_segmentation.html" class="nav-list-link">Selfie Segmentation</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/hair_segmentation.html" class="nav-list-link">Hair Segmentation</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/object_detection.html" class="nav-list-link">Object Detection</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/box_tracking.html" class="nav-list-link">Box Tracking</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/instant_motion_tracking.html" class="nav-list-link">Instant Motion Tracking</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/objectron.html" class="nav-list-link">Objectron (3D Object Detection)</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/knift.html" class="nav-list-link">KNIFT (Template-based Feature Matching)</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/autoflip.html" class="nav-list-link">AutoFlip (Saliency-aware Video Cropping)</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/media_sequence.html" class="nav-list-link">Dataset Preparation with MediaSequence</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/youtube_8m.html" class="nav-list-link">YouTube-8M Feature Extraction and Model Inference</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/solutions/models.html" class="nav-list-link">Models and Model Cards</a></li></ul></li><li class="nav-list-item"><a href="https://google.github.io/mediapipe/solutions/face_mesh#" class="nav-list-expander" aria-label="toggle links in Tools category"> <svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg> </a><a href="https://google.github.io/mediapipe/tools/tools.html" class="nav-list-link">Tools</a><ul class="nav-list"><li class="nav-list-item "><a href="https://google.github.io/mediapipe/tools/visualizer.html" class="nav-list-link">Visualizer</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/tools/tracing_and_profiling.html" class="nav-list-link">Tracing and Profiling</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/tools/performance_benchmarking.html" class="nav-list-link">Performance Benchmarking</a></li></ul></li><li class="nav-list-item"><a href="https://google.github.io/mediapipe/solutions/face_mesh#" class="nav-list-expander" aria-label="toggle links in Framework Concepts category"> <svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg> </a><a href="https://google.github.io/mediapipe/framework_concepts/framework_concepts.html" class="nav-list-link">Framework Concepts</a><ul class="nav-list"><li class="nav-list-item "><a href="https://google.github.io/mediapipe/framework_concepts/calculators.html" class="nav-list-link">Calculators</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/framework_concepts/graphs.html" class="nav-list-link">Graphs</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/framework_concepts/packets.html" class="nav-list-link">Packets</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/framework_concepts/synchronization.html" class="nav-list-link">Synchronization</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/framework_concepts/gpu.html" class="nav-list-link">GPU</a></li><li class="nav-list-item "><a href="https://google.github.io/mediapipe/framework_concepts/realtime_streams.html" class="nav-list-link">Real-time Streams</a></li></ul></li><li class="nav-list-item"><a href="https://google.github.io/mediapipe/solutions/object_detection_saved_model.html" class="nav-list-link">TensorFlow/TFLite Object Detection Model</a></li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search mediapipe" aria-label="Search mediapipe" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="https://github.com/google/mediapipe" class="site-button"> MediaPipe on GitHub </a> </li> </ul> </nav> </div> <div id="main-content-wrap" class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="https://google.github.io/mediapipe/solutions/solutions.html">Solutions</a></li> <li class="breadcrumb-nav-list-item"><span>Face Mesh</span></li> </ol> </nav> <div id="main-content" class="main-content" role="main"> <h1 class="no_toc" id="mediapipe-face-mesh"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#mediapipe-face-mesh" class="anchor-heading" aria-labelledby="mediapipe-face-mesh"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> MediaPipe Face Mesh </h1> <details close=""> <summary class="text-delta"> Table of contents </summary> <ol id="markdown-toc"> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#overview" id="markdown-toc-overview">Overview</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#ml-pipeline" id="markdown-toc-ml-pipeline">ML Pipeline</a> <ol> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#models" id="markdown-toc-models">Models</a> <ol> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#face-detection-model" id="markdown-toc-face-detection-model">Face Detection Model</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#face-landmark-model" id="markdown-toc-face-landmark-model">Face Landmark Model</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#attention-mesh-model" id="markdown-toc-attention-mesh-model">Attention Mesh Model</a></li> </ol> </li> </ol> </li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#face-transform-module" id="markdown-toc-face-transform-module">Face Transform Module</a> <ol> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#key-concepts" id="markdown-toc-key-concepts">Key Concepts</a> <ol> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#metric-3d-space" id="markdown-toc-metric-3d-space">Metric 3D Space</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#canonical-face-model" id="markdown-toc-canonical-face-model">Canonical Face Model</a></li> </ol> </li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#components" id="markdown-toc-components">Components</a> <ol> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#transform-pipeline" id="markdown-toc-transform-pipeline">Transform Pipeline</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#effect-renderer" id="markdown-toc-effect-renderer">Effect Renderer</a></li> </ol> </li> </ol> </li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#solution-apis" id="markdown-toc-solution-apis">Solution APIs</a> <ol> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#configuration-options" id="markdown-toc-configuration-options">Configuration Options</a> <ol> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#static_image_mode" id="markdown-toc-static_image_mode">static_image_mode</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#max_num_faces" id="markdown-toc-max_num_faces">max_num_faces</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#refine_landmarks" id="markdown-toc-refine_landmarks">refine_landmarks</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#min_detection_confidence" id="markdown-toc-min_detection_confidence">min_detection_confidence</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#min_tracking_confidence" id="markdown-toc-min_tracking_confidence">min_tracking_confidence</a></li> </ol> </li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#output" id="markdown-toc-output">Output</a> <ol> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#multi_face_landmarks" id="markdown-toc-multi_face_landmarks">multi_face_landmarks</a></li> </ol> </li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#python-solution-api" id="markdown-toc-python-solution-api">Python Solution API</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#javascript-solution-api" id="markdown-toc-javascript-solution-api">JavaScript Solution API</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#android-solution-api" id="markdown-toc-android-solution-api">Android Solution API</a> <ol> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#camera-input" id="markdown-toc-camera-input">Camera Input</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#image-input" id="markdown-toc-image-input">Image Input</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#video-input" id="markdown-toc-video-input">Video Input</a></li> </ol> </li> </ol> </li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#example-apps" id="markdown-toc-example-apps">Example Apps</a> <ol> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#face-landmark-example" id="markdown-toc-face-landmark-example">Face Landmark Example</a> <ol> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#mobile" id="markdown-toc-mobile">Mobile</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#desktop" id="markdown-toc-desktop">Desktop</a></li> </ol> </li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#face-effect-example" id="markdown-toc-face-effect-example">Face Effect Example</a> <ol> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#mobile-1" id="markdown-toc-mobile-1">Mobile</a></li> </ol> </li> </ol> </li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#resources" id="markdown-toc-resources">Resources</a></li> </ol> </details><hr> <p><strong>Attention:</strong> <em>Thank you for your interest in MediaPipe Solutions. As of March 1, 2023, this solution is planned to be upgraded to a new MediaPipe Solution. For more information, see the new <a href="https://developers.google.com/mediapipe/solutions/guide#legacy">MediaPipe Solutions</a> site.</em></p> <p><em>This notice and web page will be removed on April 3, 2023.</em></p><hr> <h2 id="overview"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#overview" class="anchor-heading" aria-labelledby="overview"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Overview </h2> <p>MediaPipe Face Mesh is a solution that estimates 468 3D face landmarks in real-time even on mobile devices. It employs machine learning (ML) to infer the 3D facial surface, requiring only a single camera input without the need for a dedicated depth sensor. Utilizing lightweight model architectures together with GPU acceleration throughout the pipeline, the solution delivers real-time performance critical for live experiences.</p> <p>Additionally, the solution is bundled with the Face Transform module that bridges the gap between the face landmark estimation and useful real-time augmented reality (AR) applications. It establishes a metric 3D space and uses the face landmark screen positions to estimate a face transform within that space. The face transform data consists of common 3D primitives, including a face pose transformation matrix and a triangular face mesh. Under the hood, a lightweight statistical analysis method called <a href="https://en.wikipedia.org/wiki/Procrustes_analysis">Procrustes Analysis</a> is employed to drive a robust, performant and portable logic. The analysis runs on CPU and has a minimal speed/memory footprint on top of the ML model inference.</p> <div class="table-wrapper"><table> <thead> <tr> <th style="text-align: center"><img src="./Face Mesh _ mediapipe_files/face_mesh_ar_effects.gif" alt="face_mesh_ar_effects.gif"></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Fig 1. AR effects utilizing the 3D facial surface.</em></td> </tr> </tbody> </table></div> <h2 id="ml-pipeline"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#ml-pipeline" class="anchor-heading" aria-labelledby="ml-pipeline"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> ML Pipeline </h2> <p>Our ML pipeline consists of two real-time deep neural network models that work together: A detector that operates on the full image and computes face locations and a 3D face landmark model that operates on those locations and predicts the approximate 3D surface via regression. Having the face accurately cropped drastically reduces the need for common data augmentations like affine transformations consisting of rotations, translation and scale changes. Instead it allows the network to dedicate most of its capacity towards coordinate prediction accuracy. In addition, in our pipeline the crops can also be generated based on the face landmarks identified in the previous frame, and only when the landmark model could no longer identify face presence is the face detector invoked to relocalize the face. This strategy is similar to that employed in our <a href="https://google.github.io/mediapipe/solutions/hands.html">MediaPipe Hands</a> solution, which uses a palm detector together with a hand landmark model.</p> <p>The pipeline is implemented as a MediaPipe <a href="https://github.com/google/mediapipe/tree/master/mediapipe/graphs/face_mesh/face_mesh_mobile.pbtxt">graph</a> that uses a <a href="https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_landmark/face_landmark_front_gpu.pbtxt">face landmark subgraph</a> from the <a href="https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_landmark">face landmark module</a>, and renders using a dedicated <a href="https://github.com/google/mediapipe/tree/master/mediapipe/graphs/face_mesh/subgraphs/face_renderer_gpu.pbtxt">face renderer subgraph</a>. The <a href="https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_landmark/face_landmark_front_gpu.pbtxt">face landmark subgraph</a> internally uses a <a href="https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_detection/face_detection_short_range_gpu.pbtxt">face_detection_subgraph</a> from the <a href="https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_detection">face detection module</a>.</p> <p>Note: To visualize a graph, copy the graph and paste it into <a href="https://viz.mediapipe.dev/">MediaPipe Visualizer</a>. For more information on how to visualize its associated subgraphs, please see <a href="https://google.github.io/mediapipe/tools/visualizer.html">visualizer documentation</a>.</p> <h3 id="models"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#models" class="anchor-heading" aria-labelledby="models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Models </h3> <h4 id="face-detection-model"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#face-detection-model" class="anchor-heading" aria-labelledby="face-detection-model"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Face Detection Model </h4> <p>The face detector is the same <a href="https://arxiv.org/abs/1907.05047">BlazeFace</a> model used in <a href="https://google.github.io/mediapipe/solutions/face_detection.html">MediaPipe Face Detection</a>. Please refer to <a href="https://google.github.io/mediapipe/solutions/face_detection.html">MediaPipe Face Detection</a> for details.</p> <h4 id="face-landmark-model"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#face-landmark-model" class="anchor-heading" aria-labelledby="face-landmark-model"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Face Landmark Model </h4> <p>For 3D face landmarks we employed transfer learning and trained a network with several objectives: the network simultaneously predicts 3D landmark coordinates on synthetic rendered data and 2D semantic contours on annotated real-world data. The resulting network provided us with reasonable 3D landmark predictions not just on synthetic but also on real-world data.</p> <p>The 3D landmark network receives as input a cropped video frame without additional depth input. The model outputs the positions of the 3D points, as well as the probability of a face being present and reasonably aligned in the input. A common alternative approach is to predict a 2D heatmap for each landmark, but it is not amenable to depth prediction and has high computational costs for so many points. We further improve the accuracy and robustness of our model by iteratively bootstrapping and refining predictions. That way we can grow our dataset to increasingly challenging cases, such as grimaces, oblique angle and occlusions.</p> <p>You can find more information about the face landmark model in this <a href="https://arxiv.org/abs/1907.06724">paper</a>.</p> <div class="table-wrapper"><table> <thead> <tr> <th style="text-align: center"><img src="./Face Mesh _ mediapipe_files/face_mesh_android_gpu.gif" alt="face_mesh_android_gpu.gif"></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Fig 2. Face landmarks: the red box indicates the cropped area as input to the landmark model, the red dots represent the 468 landmarks in 3D, and the green lines connecting landmarks illustrate the contours around the eyes, eyebrows, lips and the entire face.</em></td> </tr> </tbody> </table></div> <h4 id="attention-mesh-model"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#attention-mesh-model" class="anchor-heading" aria-labelledby="attention-mesh-model"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Attention Mesh Model </h4> <p>In addition to the <a href="https://google.github.io/mediapipe/solutions/face_mesh#face-landmark-model">Face Landmark Model</a> we provide another model that applies <a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)">attention</a> to semantically meaningful face regions, and therefore predicting landmarks more accurately around lips, eyes and irises, at the expense of more compute. It enables applications like AR makeup and AR puppeteering.</p> <p>The attention mesh model can be selected in the Solution APIs via the <a href="https://google.github.io/mediapipe/solutions/face_mesh#refine_landmarks">refine_landmarks</a> option. You can also find more information about the model in this <a href="https://arxiv.org/abs/2006.10962">paper</a>.</p> <div class="table-wrapper"><table> <thead> <tr> <th style="text-align: center"><img src="./Face Mesh _ mediapipe_files/attention_mesh_architecture.png" alt="attention_mesh_architecture.png"></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Fig 3. Attention Mesh: Overview of model architecture.</em></td> </tr> </tbody> </table></div> <h2 id="face-transform-module"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#face-transform-module" class="anchor-heading" aria-labelledby="face-transform-module"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Face Transform Module </h2> <p>The <a href="https://google.github.io/mediapipe/solutions/face_mesh#face-landmark-model">Face Landmark Model</a> performs a single-camera face landmark detection in the screen coordinate space: the X- and Y- coordinates are normalized screen coordinates, while the Z coordinate is relative and is scaled as the X coordinate under the <a href="https://en.wikipedia.org/wiki/3D_projection#Weak_perspective_projection">weak perspective projection camera model</a>. This format is well-suited for some applications, however it does not directly enable the full spectrum of augmented reality (AR) features like aligning a virtual 3D object with a detected face.</p> <p>The <a href="https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_geometry">Face Transform module</a> moves away from the screen coordinate space towards a metric 3D space and provides necessary primitives to handle a detected face as a regular 3D object. By design, you’ll be able to use a perspective camera to project the final 3D scene back into the screen coordinate space with a guarantee that the face landmark positions are not changed.</p> <h3 id="key-concepts"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#key-concepts" class="anchor-heading" aria-labelledby="key-concepts"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Key Concepts </h3> <h4 id="metric-3d-space"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#metric-3d-space" class="anchor-heading" aria-labelledby="metric-3d-space"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Metric 3D Space </h4> <p>The <strong>Metric 3D space</strong> established within the Face Transform module is a right-handed orthonormal metric 3D coordinate space. Within the space, there is a <strong>virtual perspective camera</strong> located at the space origin and pointed in the negative direction of the Z-axis. In the current pipeline, it is assumed that the input camera frames are observed by exactly this virtual camera and therefore its parameters are later used to convert the screen landmark coordinates back into the Metric 3D space. The <em>virtual camera parameters</em> can be set freely, however for better results it is advised to set them as close to the <em>real physical camera parameters</em> as possible.</p> <div class="table-wrapper"><table> <thead> <tr> <th style="text-align: center"><img src="./Face Mesh _ mediapipe_files/face_geometry_metric_3d_space.gif" alt="face_geometry_metric_3d_space.gif"></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Fig 4. A visualization of multiple key elements in the Metric 3D space.</em></td> </tr> </tbody> </table></div> <h4 id="canonical-face-model"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#canonical-face-model" class="anchor-heading" aria-labelledby="canonical-face-model"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Canonical Face Model </h4> <p>The <strong>Canonical Face Model</strong> is a static 3D model of a human face, which follows the 468 3D face landmark topology of the <a href="https://google.github.io/mediapipe/solutions/face_mesh#face-landmark-model">Face Landmark Model</a>. The model bears two important functions:</p> <ul> <li><strong>Defines metric units</strong>: the scale of the canonical face model defines the metric units of the Metric 3D space. A metric unit used by the <a href="https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_geometry/data/canonical_face_model.fbx">default canonical face model</a> is a centimeter;</li> <li><strong>Bridges static and runtime spaces</strong>: the face pose transformation matrix is - in fact - a linear map from the canonical face model into the runtime face landmark set estimated on each frame. This way, virtual 3D assets modeled around the canonical face model can be aligned with a tracked face by applying the face pose transformation matrix to them.</li> </ul> <h3 id="components"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#components" class="anchor-heading" aria-labelledby="components"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Components </h3> <h4 id="transform-pipeline"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#transform-pipeline" class="anchor-heading" aria-labelledby="transform-pipeline"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Transform Pipeline </h4> <p>The <strong>Transform Pipeline</strong> is a key component, which is responsible for estimating the face transform objects within the Metric 3D space. On each frame, the following steps are executed in the given order:</p> <ul> <li>Face landmark screen coordinates are converted into the Metric 3D space coordinates;</li> <li>Face pose transformation matrix is estimated as a rigid linear mapping from the canonical face metric landmark set into the runtime face metric landmark set in a way that minimizes a difference between the two;</li> <li>A face mesh is created using the runtime face metric landmarks as the vertex positions (XYZ), while both the vertex texture coordinates (UV) and the triangular topology are inherited from the canonical face model.</li> </ul> <p>The transform pipeline is implemented as a MediaPipe <a href="https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_geometry/geometry_pipeline_calculator.cc">calculator</a>. For your convenience, this calculator is bundled together with corresponding metadata into a unified MediaPipe <a href="https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_geometry/face_geometry_from_landmarks.pbtxt">subgraph</a>. The face transform format is defined as a Protocol Buffer <a href="https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_geometry/protos/face_geometry.proto">message</a>.</p> <h4 id="effect-renderer"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#effect-renderer" class="anchor-heading" aria-labelledby="effect-renderer"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Effect Renderer </h4> <p>The <strong>Effect Renderer</strong> is a component, which serves as a working example of a face effect renderer. It targets the <em>OpenGL ES 2.0</em> API to enable a real-time performance on mobile devices and supports the following rendering modes:</p> <ul> <li><strong>3D object rendering mode</strong>: a virtual object is aligned with a detected face to emulate an object attached to the face (example: glasses);</li> <li><strong>Face mesh rendering mode</strong>: a texture is stretched on top of the face mesh surface to emulate a face painting technique.</li> </ul> <p>In both rendering modes, the face mesh is first rendered as an occluder straight into the depth buffer. This step helps to create a more believable effect via hiding invisible elements behind the face surface.</p> <p>The effect renderer is implemented as a MediaPipe <a href="https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_geometry/effect_renderer_calculator.cc">calculator</a>.</p> <div class="table-wrapper"><table> <thead> <tr> <th style="text-align: center"><img src="./Face Mesh _ mediapipe_files/face_geometry_renderer.gif" alt="face_geometry_renderer.gif"></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Fig 5. An example of face effects rendered by the Face Transform Effect Renderer.</em></td> </tr> </tbody> </table></div> <h2 id="solution-apis"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#solution-apis" class="anchor-heading" aria-labelledby="solution-apis"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Solution APIs </h2> <h3 id="configuration-options"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#configuration-options" class="anchor-heading" aria-labelledby="configuration-options"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Configuration Options </h3> <p>Naming style and availability may differ slightly across platforms/languages.</p> <h4 id="static_image_mode"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#static_image_mode" class="anchor-heading" aria-labelledby="static_image_mode"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> static_image_mode </h4> <p>If set to <code class="language-plaintext highlighter-rouge">false</code>, the solution treats the input images as a video stream. It will try to detect faces in the first input images, and upon a successful detection further localizes the face landmarks. In subsequent images, once all <a href="https://google.github.io/mediapipe/solutions/face_mesh#max_num_faces">max_num_faces</a> faces are detected and the corresponding face landmarks are localized, it simply tracks those landmarks without invoking another detection until it loses track of any of the faces. This reduces latency and is ideal for processing video frames. If set to <code class="language-plaintext highlighter-rouge">true</code>, face detection runs on every input image, ideal for processing a batch of static, possibly unrelated, images. Default to <code class="language-plaintext highlighter-rouge">false</code>.</p> <h4 id="max_num_faces"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#max_num_faces" class="anchor-heading" aria-labelledby="max_num_faces"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> max_num_faces </h4> <p>Maximum number of faces to detect. Default to <code class="language-plaintext highlighter-rouge">1</code>.</p> <h4 id="refine_landmarks"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#refine_landmarks" class="anchor-heading" aria-labelledby="refine_landmarks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> refine_landmarks </h4> <p>Whether to further refine the landmark coordinates around the eyes and lips, and output additional landmarks around the irises by applying the <a href="https://google.github.io/mediapipe/solutions/face_mesh#attention-mesh-model">Attention Mesh Model</a>. Default to <code class="language-plaintext highlighter-rouge">false</code>.</p> <h4 id="min_detection_confidence"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#min_detection_confidence" class="anchor-heading" aria-labelledby="min_detection_confidence"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> min_detection_confidence </h4> <p>Minimum confidence value (<code class="language-plaintext highlighter-rouge">[0.0, 1.0]</code>) from the face detection model for the detection to be considered successful. Default to <code class="language-plaintext highlighter-rouge">0.5</code>.</p> <h4 id="min_tracking_confidence"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#min_tracking_confidence" class="anchor-heading" aria-labelledby="min_tracking_confidence"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> min_tracking_confidence </h4> <p>Minimum confidence value (<code class="language-plaintext highlighter-rouge">[0.0, 1.0]</code>) from the landmark-tracking model for the face landmarks to be considered tracked successfully, or otherwise face detection will be invoked automatically on the next input image. Setting it to a higher value can increase robustness of the solution, at the expense of a higher latency. Ignored if <a href="https://google.github.io/mediapipe/solutions/face_mesh#static_image_mode">static_image_mode</a> is <code class="language-plaintext highlighter-rouge">true</code>, where face detection simply runs on every image. Default to <code class="language-plaintext highlighter-rouge">0.5</code>.</p> <h3 id="output"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#output" class="anchor-heading" aria-labelledby="output"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Output </h3> <p>Naming style may differ slightly across platforms/languages.</p> <h4 id="multi_face_landmarks"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#multi_face_landmarks" class="anchor-heading" aria-labelledby="multi_face_landmarks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> multi_face_landmarks </h4> <p>Collection of detected/tracked faces, where each face is represented as a list of 468 face landmarks and each landmark is composed of <code class="language-plaintext highlighter-rouge">x</code>, <code class="language-plaintext highlighter-rouge">y</code> and <code class="language-plaintext highlighter-rouge">z</code>. <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> are normalized to <code class="language-plaintext highlighter-rouge">[0.0, 1.0]</code> by the image width and height respectively. <code class="language-plaintext highlighter-rouge">z</code> represents the landmark depth with the depth at center of the head being the origin, and the smaller the value the closer the landmark is to the camera. The magnitude of <code class="language-plaintext highlighter-rouge">z</code> uses roughly the same scale as <code class="language-plaintext highlighter-rouge">x</code>.</p> <h3 id="python-solution-api"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#python-solution-api" class="anchor-heading" aria-labelledby="python-solution-api"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Python Solution API </h3> <p>Please first follow general <a href="https://google.github.io/mediapipe/getting_started/python.html">instructions</a> to install MediaPipe Python package, then learn more in the companion <a href="https://google.github.io/mediapipe/solutions/face_mesh#resources">Python Colab</a> and the usage example below.</p> <p>Supported configuration options:</p> <ul> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#static_image_mode">static_image_mode</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#max_num_faces">max_num_faces</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#refine_landmarks">refine_landmarks</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#min_detection_confidence">min_detection_confidence</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#min_tracking_confidence">min_tracking_confidence</a></li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">mediapipe</span> <span class="k">as</span> <span class="n">mp</span>
<span class="n">mp_drawing</span> <span class="o">=</span> <span class="n">mp</span><span class="p">.</span><span class="n">solutions</span><span class="p">.</span><span class="n">drawing_utils</span>
<span class="n">mp_drawing_styles</span> <span class="o">=</span> <span class="n">mp</span><span class="p">.</span><span class="n">solutions</span><span class="p">.</span><span class="n">drawing_styles</span>
<span class="n">mp_face_mesh</span> <span class="o">=</span> <span class="n">mp</span><span class="p">.</span><span class="n">solutions</span><span class="p">.</span><span class="n">face_mesh</span>

<span class="c1"># For static images:
</span><span class="n">IMAGE_FILES</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">drawing_spec</span> <span class="o">=</span> <span class="n">mp_drawing</span><span class="p">.</span><span class="n">DrawingSpec</span><span class="p">(</span><span class="n">thickness</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">circle_radius</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">with</span> <span class="n">mp_face_mesh</span><span class="p">.</span><span class="n">FaceMesh</span><span class="p">(</span>
    <span class="n">static_image_mode</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">max_num_faces</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">refine_landmarks</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">min_detection_confidence</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="k">as</span> <span class="n">face_mesh</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="nb">file</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">IMAGE_FILES</span><span class="p">):</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">imread</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>
    <span class="c1"># Convert the BGR image to RGB before processing.
</span>    <span class="n">results</span> <span class="o">=</span> <span class="n">face_mesh</span><span class="p">.</span><span class="n">process</span><span class="p">(</span><span class="n">cv2</span><span class="p">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">))</span>

    <span class="c1"># Print and draw face mesh landmarks on the image.
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">results</span><span class="p">.</span><span class="n">multi_face_landmarks</span><span class="p">:</span>
      <span class="k">continue</span>
    <span class="n">annotated_image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">face_landmarks</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="n">multi_face_landmarks</span><span class="p">:</span>
      <span class="k">print</span><span class="p">(</span><span class="s">'face_landmarks:'</span><span class="p">,</span> <span class="n">face_landmarks</span><span class="p">)</span>
      <span class="n">mp_drawing</span><span class="p">.</span><span class="n">draw_landmarks</span><span class="p">(</span>
          <span class="n">image</span><span class="o">=</span><span class="n">annotated_image</span><span class="p">,</span>
          <span class="n">landmark_list</span><span class="o">=</span><span class="n">face_landmarks</span><span class="p">,</span>
          <span class="n">connections</span><span class="o">=</span><span class="n">mp_face_mesh</span><span class="p">.</span><span class="n">FACEMESH_TESSELATION</span><span class="p">,</span>
          <span class="n">landmark_drawing_spec</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
          <span class="n">connection_drawing_spec</span><span class="o">=</span><span class="n">mp_drawing_styles</span>
          <span class="p">.</span><span class="n">get_default_face_mesh_tesselation_style</span><span class="p">())</span>
      <span class="n">mp_drawing</span><span class="p">.</span><span class="n">draw_landmarks</span><span class="p">(</span>
          <span class="n">image</span><span class="o">=</span><span class="n">annotated_image</span><span class="p">,</span>
          <span class="n">landmark_list</span><span class="o">=</span><span class="n">face_landmarks</span><span class="p">,</span>
          <span class="n">connections</span><span class="o">=</span><span class="n">mp_face_mesh</span><span class="p">.</span><span class="n">FACEMESH_CONTOURS</span><span class="p">,</span>
          <span class="n">landmark_drawing_spec</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
          <span class="n">connection_drawing_spec</span><span class="o">=</span><span class="n">mp_drawing_styles</span>
          <span class="p">.</span><span class="n">get_default_face_mesh_contours_style</span><span class="p">())</span>
      <span class="n">mp_drawing</span><span class="p">.</span><span class="n">draw_landmarks</span><span class="p">(</span>
          <span class="n">image</span><span class="o">=</span><span class="n">annotated_image</span><span class="p">,</span>
          <span class="n">landmark_list</span><span class="o">=</span><span class="n">face_landmarks</span><span class="p">,</span>
          <span class="n">connections</span><span class="o">=</span><span class="n">mp_face_mesh</span><span class="p">.</span><span class="n">FACEMESH_IRISES</span><span class="p">,</span>
          <span class="n">landmark_drawing_spec</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
          <span class="n">connection_drawing_spec</span><span class="o">=</span><span class="n">mp_drawing_styles</span>
          <span class="p">.</span><span class="n">get_default_face_mesh_iris_connections_style</span><span class="p">())</span>
    <span class="n">cv2</span><span class="p">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s">'/tmp/annotated_image'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="o">+</span> <span class="s">'.png'</span><span class="p">,</span> <span class="n">annotated_image</span><span class="p">)</span>

<span class="c1"># For webcam input:
</span><span class="n">drawing_spec</span> <span class="o">=</span> <span class="n">mp_drawing</span><span class="p">.</span><span class="n">DrawingSpec</span><span class="p">(</span><span class="n">thickness</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">circle_radius</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">with</span> <span class="n">mp_face_mesh</span><span class="p">.</span><span class="n">FaceMesh</span><span class="p">(</span>
    <span class="n">max_num_faces</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">refine_landmarks</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">min_detection_confidence</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">min_tracking_confidence</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="k">as</span> <span class="n">face_mesh</span><span class="p">:</span>
  <span class="k">while</span> <span class="n">cap</span><span class="p">.</span><span class="n">isOpened</span><span class="p">():</span>
    <span class="n">success</span><span class="p">,</span> <span class="n">image</span> <span class="o">=</span> <span class="n">cap</span><span class="p">.</span><span class="n">read</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">success</span><span class="p">:</span>
      <span class="k">print</span><span class="p">(</span><span class="s">"Ignoring empty camera frame."</span><span class="p">)</span>
      <span class="c1"># If loading a video, use 'break' instead of 'continue'.
</span>      <span class="k">continue</span>

    <span class="c1"># To improve performance, optionally mark the image as not writeable to
</span>    <span class="c1"># pass by reference.
</span>    <span class="n">image</span><span class="p">.</span><span class="n">flags</span><span class="p">.</span><span class="n">writeable</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">face_mesh</span><span class="p">.</span><span class="n">process</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

    <span class="c1"># Draw the face mesh annotations on the image.
</span>    <span class="n">image</span><span class="p">.</span><span class="n">flags</span><span class="p">.</span><span class="n">writeable</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_RGB2BGR</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">results</span><span class="p">.</span><span class="n">multi_face_landmarks</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">face_landmarks</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="n">multi_face_landmarks</span><span class="p">:</span>
        <span class="n">mp_drawing</span><span class="p">.</span><span class="n">draw_landmarks</span><span class="p">(</span>
            <span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="p">,</span>
            <span class="n">landmark_list</span><span class="o">=</span><span class="n">face_landmarks</span><span class="p">,</span>
            <span class="n">connections</span><span class="o">=</span><span class="n">mp_face_mesh</span><span class="p">.</span><span class="n">FACEMESH_TESSELATION</span><span class="p">,</span>
            <span class="n">landmark_drawing_spec</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
            <span class="n">connection_drawing_spec</span><span class="o">=</span><span class="n">mp_drawing_styles</span>
            <span class="p">.</span><span class="n">get_default_face_mesh_tesselation_style</span><span class="p">())</span>
        <span class="n">mp_drawing</span><span class="p">.</span><span class="n">draw_landmarks</span><span class="p">(</span>
            <span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="p">,</span>
            <span class="n">landmark_list</span><span class="o">=</span><span class="n">face_landmarks</span><span class="p">,</span>
            <span class="n">connections</span><span class="o">=</span><span class="n">mp_face_mesh</span><span class="p">.</span><span class="n">FACEMESH_CONTOURS</span><span class="p">,</span>
            <span class="n">landmark_drawing_spec</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
            <span class="n">connection_drawing_spec</span><span class="o">=</span><span class="n">mp_drawing_styles</span>
            <span class="p">.</span><span class="n">get_default_face_mesh_contours_style</span><span class="p">())</span>
        <span class="n">mp_drawing</span><span class="p">.</span><span class="n">draw_landmarks</span><span class="p">(</span>
            <span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="p">,</span>
            <span class="n">landmark_list</span><span class="o">=</span><span class="n">face_landmarks</span><span class="p">,</span>
            <span class="n">connections</span><span class="o">=</span><span class="n">mp_face_mesh</span><span class="p">.</span><span class="n">FACEMESH_IRISES</span><span class="p">,</span>
            <span class="n">landmark_drawing_spec</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
            <span class="n">connection_drawing_spec</span><span class="o">=</span><span class="n">mp_drawing_styles</span>
            <span class="p">.</span><span class="n">get_default_face_mesh_iris_connections_style</span><span class="p">())</span>
    <span class="c1"># Flip the image horizontally for a selfie-view display.
</span>    <span class="n">cv2</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="s">'MediaPipe Face Mesh'</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">flip</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">cv2</span><span class="p">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xFF</span> <span class="o">==</span> <span class="mi">27</span><span class="p">:</span>
      <span class="k">break</span>
<span class="n">cap</span><span class="p">.</span><span class="n">release</span><span class="p">()</span>
</code></pre></div><button type="button" aria-label="Copy code to clipboard"><svg viewBox="0 0 24 24" class="copy-icon"><use xlink:href="#svg-copy"></use></svg></button></div> <h3 id="javascript-solution-api"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#javascript-solution-api" class="anchor-heading" aria-labelledby="javascript-solution-api"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> JavaScript Solution API </h3> <p>Please first see general <a href="https://google.github.io/mediapipe/getting_started/javascript.html">introduction</a> on MediaPipe in JavaScript, then learn more in the companion <a href="https://google.github.io/mediapipe/solutions/face_mesh#resources">web demo</a> and the following usage example.</p> <p>Supported configuration options:</p> <ul> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#max_num_faces">maxNumFaces</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#refine_landmarks">refineLandmarks</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#min_detection_confidence">minDetectionConfidence</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#min_tracking_confidence">minTrackingConfidence</a></li> </ul> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">&lt;!DOCTYPE html&gt;</span>
<span class="nt">&lt;html&gt;</span>
<span class="nt">&lt;head&gt;</span>
  <span class="nt">&lt;meta</span> <span class="na">charset=</span><span class="s">"utf-8"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;script </span><span class="na">src=</span><span class="s">"https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"</span> <span class="na">crossorigin=</span><span class="s">"anonymous"</span><span class="nt">&gt;&lt;/script&gt;</span>
  <span class="nt">&lt;script </span><span class="na">src=</span><span class="s">"https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js"</span> <span class="na">crossorigin=</span><span class="s">"anonymous"</span><span class="nt">&gt;&lt;/script&gt;</span>
  <span class="nt">&lt;script </span><span class="na">src=</span><span class="s">"https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"</span> <span class="na">crossorigin=</span><span class="s">"anonymous"</span><span class="nt">&gt;&lt;/script&gt;</span>
  <span class="nt">&lt;script </span><span class="na">src=</span><span class="s">"https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"</span> <span class="na">crossorigin=</span><span class="s">"anonymous"</span><span class="nt">&gt;&lt;/script&gt;</span>
<span class="nt">&lt;/head&gt;</span>

<span class="nt">&lt;body&gt;</span>
  <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"container"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;video</span> <span class="na">class=</span><span class="s">"input_video"</span><span class="nt">&gt;&lt;/video&gt;</span>
    <span class="nt">&lt;canvas</span> <span class="na">class=</span><span class="s">"output_canvas"</span> <span class="na">width=</span><span class="s">"1280px"</span> <span class="na">height=</span><span class="s">"720px"</span><span class="nt">&gt;&lt;/canvas&gt;</span>
  <span class="nt">&lt;/div&gt;</span>
<span class="nt">&lt;/body&gt;</span>
<span class="nt">&lt;/html&gt;</span>
</code></pre></div><button type="button" aria-label="Copy code to clipboard"><svg viewBox="0 0 24 24" class="copy-icon"><use xlink:href="#svg-copy"></use></svg></button></div> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&lt;</span><span class="nx">script</span> <span class="nx">type</span><span class="o">=</span><span class="dl">"</span><span class="s2">module</span><span class="dl">"</span><span class="o">&gt;</span>
<span class="kd">const</span> <span class="nx">videoElement</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nx">getElementsByClassName</span><span class="p">(</span><span class="dl">'</span><span class="s1">input_video</span><span class="dl">'</span><span class="p">)[</span><span class="mi">0</span><span class="p">];</span>
<span class="kd">const</span> <span class="nx">canvasElement</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nx">getElementsByClassName</span><span class="p">(</span><span class="dl">'</span><span class="s1">output_canvas</span><span class="dl">'</span><span class="p">)[</span><span class="mi">0</span><span class="p">];</span>
<span class="kd">const</span> <span class="nx">canvasCtx</span> <span class="o">=</span> <span class="nx">canvasElement</span><span class="p">.</span><span class="nx">getContext</span><span class="p">(</span><span class="dl">'</span><span class="s1">2d</span><span class="dl">'</span><span class="p">);</span>

<span class="kd">function</span> <span class="nx">onResults</span><span class="p">(</span><span class="nx">results</span><span class="p">)</span> <span class="p">{</span>
  <span class="nx">canvasCtx</span><span class="p">.</span><span class="nx">save</span><span class="p">();</span>
  <span class="nx">canvasCtx</span><span class="p">.</span><span class="nx">clearRect</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nx">canvasElement</span><span class="p">.</span><span class="nx">width</span><span class="p">,</span> <span class="nx">canvasElement</span><span class="p">.</span><span class="nx">height</span><span class="p">);</span>
  <span class="nx">canvasCtx</span><span class="p">.</span><span class="nx">drawImage</span><span class="p">(</span>
      <span class="nx">results</span><span class="p">.</span><span class="nx">image</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nx">canvasElement</span><span class="p">.</span><span class="nx">width</span><span class="p">,</span> <span class="nx">canvasElement</span><span class="p">.</span><span class="nx">height</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="nx">results</span><span class="p">.</span><span class="nx">multiFaceLandmarks</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kd">const</span> <span class="nx">landmarks</span> <span class="k">of</span> <span class="nx">results</span><span class="p">.</span><span class="nx">multiFaceLandmarks</span><span class="p">)</span> <span class="p">{</span>
      <span class="nx">drawConnectors</span><span class="p">(</span><span class="nx">canvasCtx</span><span class="p">,</span> <span class="nx">landmarks</span><span class="p">,</span> <span class="nx">FACEMESH_TESSELATION</span><span class="p">,</span>
                     <span class="p">{</span><span class="na">color</span><span class="p">:</span> <span class="dl">'</span><span class="s1">#C0C0C070</span><span class="dl">'</span><span class="p">,</span> <span class="na">lineWidth</span><span class="p">:</span> <span class="mi">1</span><span class="p">});</span>
      <span class="nx">drawConnectors</span><span class="p">(</span><span class="nx">canvasCtx</span><span class="p">,</span> <span class="nx">landmarks</span><span class="p">,</span> <span class="nx">FACEMESH_RIGHT_EYE</span><span class="p">,</span> <span class="p">{</span><span class="na">color</span><span class="p">:</span> <span class="dl">'</span><span class="s1">#FF3030</span><span class="dl">'</span><span class="p">});</span>
      <span class="nx">drawConnectors</span><span class="p">(</span><span class="nx">canvasCtx</span><span class="p">,</span> <span class="nx">landmarks</span><span class="p">,</span> <span class="nx">FACEMESH_RIGHT_EYEBROW</span><span class="p">,</span> <span class="p">{</span><span class="na">color</span><span class="p">:</span> <span class="dl">'</span><span class="s1">#FF3030</span><span class="dl">'</span><span class="p">});</span>
      <span class="nx">drawConnectors</span><span class="p">(</span><span class="nx">canvasCtx</span><span class="p">,</span> <span class="nx">landmarks</span><span class="p">,</span> <span class="nx">FACEMESH_RIGHT_IRIS</span><span class="p">,</span> <span class="p">{</span><span class="na">color</span><span class="p">:</span> <span class="dl">'</span><span class="s1">#FF3030</span><span class="dl">'</span><span class="p">});</span>
      <span class="nx">drawConnectors</span><span class="p">(</span><span class="nx">canvasCtx</span><span class="p">,</span> <span class="nx">landmarks</span><span class="p">,</span> <span class="nx">FACEMESH_LEFT_EYE</span><span class="p">,</span> <span class="p">{</span><span class="na">color</span><span class="p">:</span> <span class="dl">'</span><span class="s1">#30FF30</span><span class="dl">'</span><span class="p">});</span>
      <span class="nx">drawConnectors</span><span class="p">(</span><span class="nx">canvasCtx</span><span class="p">,</span> <span class="nx">landmarks</span><span class="p">,</span> <span class="nx">FACEMESH_LEFT_EYEBROW</span><span class="p">,</span> <span class="p">{</span><span class="na">color</span><span class="p">:</span> <span class="dl">'</span><span class="s1">#30FF30</span><span class="dl">'</span><span class="p">});</span>
      <span class="nx">drawConnectors</span><span class="p">(</span><span class="nx">canvasCtx</span><span class="p">,</span> <span class="nx">landmarks</span><span class="p">,</span> <span class="nx">FACEMESH_LEFT_IRIS</span><span class="p">,</span> <span class="p">{</span><span class="na">color</span><span class="p">:</span> <span class="dl">'</span><span class="s1">#30FF30</span><span class="dl">'</span><span class="p">});</span>
      <span class="nx">drawConnectors</span><span class="p">(</span><span class="nx">canvasCtx</span><span class="p">,</span> <span class="nx">landmarks</span><span class="p">,</span> <span class="nx">FACEMESH_FACE_OVAL</span><span class="p">,</span> <span class="p">{</span><span class="na">color</span><span class="p">:</span> <span class="dl">'</span><span class="s1">#E0E0E0</span><span class="dl">'</span><span class="p">});</span>
      <span class="nx">drawConnectors</span><span class="p">(</span><span class="nx">canvasCtx</span><span class="p">,</span> <span class="nx">landmarks</span><span class="p">,</span> <span class="nx">FACEMESH_LIPS</span><span class="p">,</span> <span class="p">{</span><span class="na">color</span><span class="p">:</span> <span class="dl">'</span><span class="s1">#E0E0E0</span><span class="dl">'</span><span class="p">});</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="nx">canvasCtx</span><span class="p">.</span><span class="nx">restore</span><span class="p">();</span>
<span class="p">}</span>

<span class="kd">const</span> <span class="nx">faceMesh</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">FaceMesh</span><span class="p">({</span><span class="na">locateFile</span><span class="p">:</span> <span class="p">(</span><span class="nx">file</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
  <span class="k">return</span> <span class="s2">`https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/</span><span class="p">${</span><span class="nx">file</span><span class="p">}</span><span class="s2">`</span><span class="p">;</span>
<span class="p">}});</span>
<span class="nx">faceMesh</span><span class="p">.</span><span class="nx">setOptions</span><span class="p">({</span>
  <span class="na">maxNumFaces</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
  <span class="na">refineLandmarks</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="na">minDetectionConfidence</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
  <span class="na">minTrackingConfidence</span><span class="p">:</span> <span class="mf">0.5</span>
<span class="p">});</span>
<span class="nx">faceMesh</span><span class="p">.</span><span class="nx">onResults</span><span class="p">(</span><span class="nx">onResults</span><span class="p">);</span>

<span class="kd">const</span> <span class="nx">camera</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">Camera</span><span class="p">(</span><span class="nx">videoElement</span><span class="p">,</span> <span class="p">{</span>
  <span class="na">onFrame</span><span class="p">:</span> <span class="k">async</span> <span class="p">()</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="k">await</span> <span class="nx">faceMesh</span><span class="p">.</span><span class="nx">send</span><span class="p">({</span><span class="na">image</span><span class="p">:</span> <span class="nx">videoElement</span><span class="p">});</span>
  <span class="p">},</span>
  <span class="na">width</span><span class="p">:</span> <span class="mi">1280</span><span class="p">,</span>
  <span class="na">height</span><span class="p">:</span> <span class="mi">720</span>
<span class="p">});</span>
<span class="nx">camera</span><span class="p">.</span><span class="nx">start</span><span class="p">();</span>
<span class="o">&lt;</span><span class="sr">/script</span><span class="err">&gt;
</span></code></pre></div><button type="button" aria-label="Copy code to clipboard"><svg viewBox="0 0 24 24" class="copy-icon"><use xlink:href="#svg-copy"></use></svg></button></div> <h3 id="android-solution-api"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#android-solution-api" class="anchor-heading" aria-labelledby="android-solution-api"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Android Solution API </h3> <p>Please first follow general <a href="https://google.github.io/mediapipe/getting_started/android_solutions.html">instructions</a> to add MediaPipe Gradle dependencies and try the Android Solution API in the companion <a href="https://github.com/google/mediapipe/tree/master/mediapipe/examples/android/solutions/facemesh">example Android Studio project</a>, and learn more in the usage example below.</p> <p>Supported configuration options:</p> <ul> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#static_image_mode">staticImageMode</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#max_num_faces">maxNumFaces</a></li> <li><a href="https://google.github.io/mediapipe/solutions/face_mesh#refine_landmarks">refineLandmarks</a></li> <li>runOnGpu: Run the pipeline and the model inference on GPU or CPU.</li> </ul> <h4 id="camera-input"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#camera-input" class="anchor-heading" aria-labelledby="camera-input"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Camera Input </h4> <div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// For camera input and result rendering with OpenGL.</span>
<span class="nc">FaceMeshOptions</span> <span class="n">faceMeshOptions</span> <span class="o">=</span>
    <span class="nc">FaceMeshOptions</span><span class="o">.</span><span class="na">builder</span><span class="o">()</span>
        <span class="o">.</span><span class="na">setStaticImageMode</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
        <span class="o">.</span><span class="na">setRefineLandmarks</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
        <span class="o">.</span><span class="na">setMaxNumFaces</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
        <span class="o">.</span><span class="na">setRunOnGpu</span><span class="o">(</span><span class="kc">true</span><span class="o">).</span><span class="na">build</span><span class="o">();</span>
<span class="nc">FaceMesh</span> <span class="n">faceMesh</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">FaceMesh</span><span class="o">(</span><span class="k">this</span><span class="o">,</span> <span class="n">faceMeshOptions</span><span class="o">);</span>
<span class="n">faceMesh</span><span class="o">.</span><span class="na">setErrorListener</span><span class="o">(</span>
    <span class="o">(</span><span class="n">message</span><span class="o">,</span> <span class="n">e</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="nc">Log</span><span class="o">.</span><span class="na">e</span><span class="o">(</span><span class="no">TAG</span><span class="o">,</span> <span class="s">"MediaPipe Face Mesh error:"</span> <span class="o">+</span> <span class="n">message</span><span class="o">));</span>

<span class="c1">// Initializes a new CameraInput instance and connects it to MediaPipe Face Mesh Solution.</span>
<span class="nc">CameraInput</span> <span class="n">cameraInput</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">CameraInput</span><span class="o">(</span><span class="k">this</span><span class="o">);</span>
<span class="n">cameraInput</span><span class="o">.</span><span class="na">setNewFrameListener</span><span class="o">(</span>
    <span class="n">textureFrame</span> <span class="o">-&gt;</span> <span class="n">faceMesh</span><span class="o">.</span><span class="na">send</span><span class="o">(</span><span class="n">textureFrame</span><span class="o">));</span>

<span class="c1">// Initializes a new GlSurfaceView with a ResultGlRenderer&lt;FaceMeshResult&gt; instance</span>
<span class="c1">// that provides the interfaces to run user-defined OpenGL rendering code.</span>
<span class="c1">// See mediapipe/examples/android/solutions/facemesh/src/main/java/com/google/mediapipe/examples/facemesh/FaceMeshResultGlRenderer.java</span>
<span class="c1">// as an example.</span>
<span class="nc">SolutionGlSurfaceView</span><span class="o">&lt;</span><span class="nc">FaceMeshResult</span><span class="o">&gt;</span> <span class="n">glSurfaceView</span> <span class="o">=</span>
    <span class="k">new</span> <span class="nc">SolutionGlSurfaceView</span><span class="o">&lt;&gt;(</span>
        <span class="k">this</span><span class="o">,</span> <span class="n">faceMesh</span><span class="o">.</span><span class="na">getGlContext</span><span class="o">(),</span> <span class="n">faceMesh</span><span class="o">.</span><span class="na">getGlMajorVersion</span><span class="o">());</span>
<span class="n">glSurfaceView</span><span class="o">.</span><span class="na">setSolutionResultRenderer</span><span class="o">(</span><span class="k">new</span> <span class="nc">FaceMeshResultGlRenderer</span><span class="o">());</span>
<span class="n">glSurfaceView</span><span class="o">.</span><span class="na">setRenderInputImage</span><span class="o">(</span><span class="kc">true</span><span class="o">);</span>

<span class="n">faceMesh</span><span class="o">.</span><span class="na">setResultListener</span><span class="o">(</span>
    <span class="n">faceMeshResult</span> <span class="o">-&gt;</span> <span class="o">{</span>
      <span class="nc">NormalizedLandmark</span> <span class="n">noseLandmark</span> <span class="o">=</span>
          <span class="n">result</span><span class="o">.</span><span class="na">multiFaceLandmarks</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="na">getLandmarkList</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="mi">1</span><span class="o">);</span>
      <span class="nc">Log</span><span class="o">.</span><span class="na">i</span><span class="o">(</span>
          <span class="no">TAG</span><span class="o">,</span>
          <span class="nc">String</span><span class="o">.</span><span class="na">format</span><span class="o">(</span>
              <span class="s">"MediaPipe Face Mesh nose normalized coordinates (value range: [0, 1]): x=%f, y=%f"</span><span class="o">,</span>
              <span class="n">noseLandmark</span><span class="o">.</span><span class="na">getX</span><span class="o">(),</span> <span class="n">noseLandmark</span><span class="o">.</span><span class="na">getY</span><span class="o">()));</span>
      <span class="c1">// Request GL rendering.</span>
      <span class="n">glSurfaceView</span><span class="o">.</span><span class="na">setRenderData</span><span class="o">(</span><span class="n">faceMeshResult</span><span class="o">);</span>
      <span class="n">glSurfaceView</span><span class="o">.</span><span class="na">requestRender</span><span class="o">();</span>
    <span class="o">});</span>

<span class="c1">// The runnable to start camera after the GLSurfaceView is attached.</span>
<span class="n">glSurfaceView</span><span class="o">.</span><span class="na">post</span><span class="o">(</span>
    <span class="o">()</span> <span class="o">-&gt;</span>
        <span class="n">cameraInput</span><span class="o">.</span><span class="na">start</span><span class="o">(</span>
            <span class="k">this</span><span class="o">,</span>
            <span class="n">faceMesh</span><span class="o">.</span><span class="na">getGlContext</span><span class="o">(),</span>
            <span class="nc">CameraInput</span><span class="o">.</span><span class="na">CameraFacing</span><span class="o">.</span><span class="na">FRONT</span><span class="o">,</span>
            <span class="n">glSurfaceView</span><span class="o">.</span><span class="na">getWidth</span><span class="o">(),</span>
            <span class="n">glSurfaceView</span><span class="o">.</span><span class="na">getHeight</span><span class="o">()));</span>
</code></pre></div><button type="button" aria-label="Copy code to clipboard"><svg viewBox="0 0 24 24" class="copy-icon"><use xlink:href="#svg-copy"></use></svg></button></div> <h4 id="image-input"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#image-input" class="anchor-heading" aria-labelledby="image-input"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Image Input </h4> <div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// For reading images from gallery and drawing the output in an ImageView.</span>
<span class="nc">FaceMeshOptions</span> <span class="n">faceMeshOptions</span> <span class="o">=</span>
    <span class="nc">FaceMeshOptions</span><span class="o">.</span><span class="na">builder</span><span class="o">()</span>
        <span class="o">.</span><span class="na">setStaticImageMode</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
        <span class="o">.</span><span class="na">setRefineLandmarks</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
        <span class="o">.</span><span class="na">setMaxNumFaces</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
        <span class="o">.</span><span class="na">setRunOnGpu</span><span class="o">(</span><span class="kc">true</span><span class="o">).</span><span class="na">build</span><span class="o">();</span>
<span class="nc">FaceMesh</span> <span class="n">faceMesh</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">FaceMesh</span><span class="o">(</span><span class="k">this</span><span class="o">,</span> <span class="n">faceMeshOptions</span><span class="o">);</span>

<span class="c1">// Connects MediaPipe Face Mesh Solution to the user-defined ImageView instance</span>
<span class="c1">// that allows users to have the custom drawing of the output landmarks on it.</span>
<span class="c1">// See mediapipe/examples/android/solutions/facemesh/src/main/java/com/google/mediapipe/examples/facemesh/FaceMeshResultImageView.java</span>
<span class="c1">// as an example.</span>
<span class="nc">FaceMeshResultImageView</span> <span class="n">imageView</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">FaceMeshResultImageView</span><span class="o">(</span><span class="k">this</span><span class="o">);</span>
<span class="n">faceMesh</span><span class="o">.</span><span class="na">setResultListener</span><span class="o">(</span>
    <span class="n">faceMeshResult</span> <span class="o">-&gt;</span> <span class="o">{</span>
      <span class="kt">int</span> <span class="n">width</span> <span class="o">=</span> <span class="n">faceMeshResult</span><span class="o">.</span><span class="na">inputBitmap</span><span class="o">().</span><span class="na">getWidth</span><span class="o">();</span>
      <span class="kt">int</span> <span class="n">height</span> <span class="o">=</span> <span class="n">faceMeshResult</span><span class="o">.</span><span class="na">inputBitmap</span><span class="o">().</span><span class="na">getHeight</span><span class="o">();</span>
      <span class="nc">NormalizedLandmark</span> <span class="n">noseLandmark</span> <span class="o">=</span>
          <span class="n">result</span><span class="o">.</span><span class="na">multiFaceLandmarks</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="na">getLandmarkList</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="mi">1</span><span class="o">);</span>
      <span class="nc">Log</span><span class="o">.</span><span class="na">i</span><span class="o">(</span>
          <span class="no">TAG</span><span class="o">,</span>
          <span class="nc">String</span><span class="o">.</span><span class="na">format</span><span class="o">(</span>
              <span class="s">"MediaPipe Face Mesh nose coordinates (pixel values): x=%f, y=%f"</span><span class="o">,</span>
              <span class="n">noseLandmark</span><span class="o">.</span><span class="na">getX</span><span class="o">()</span> <span class="o">*</span> <span class="n">width</span><span class="o">,</span> <span class="n">noseLandmark</span><span class="o">.</span><span class="na">getY</span><span class="o">()</span> <span class="o">*</span> <span class="n">height</span><span class="o">));</span>
      <span class="c1">// Request canvas drawing.</span>
      <span class="n">imageView</span><span class="o">.</span><span class="na">setFaceMeshResult</span><span class="o">(</span><span class="n">faceMeshResult</span><span class="o">);</span>
      <span class="n">runOnUiThread</span><span class="o">(()</span> <span class="o">-&gt;</span> <span class="n">imageView</span><span class="o">.</span><span class="na">update</span><span class="o">());</span>
    <span class="o">});</span>
<span class="n">faceMesh</span><span class="o">.</span><span class="na">setErrorListener</span><span class="o">(</span>
    <span class="o">(</span><span class="n">message</span><span class="o">,</span> <span class="n">e</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="nc">Log</span><span class="o">.</span><span class="na">e</span><span class="o">(</span><span class="no">TAG</span><span class="o">,</span> <span class="s">"MediaPipe Face Mesh error:"</span> <span class="o">+</span> <span class="n">message</span><span class="o">));</span>

<span class="c1">// ActivityResultLauncher to get an image from the gallery as Bitmap.</span>
<span class="nc">ActivityResultLauncher</span><span class="o">&lt;</span><span class="nc">Intent</span><span class="o">&gt;</span> <span class="n">imageGetter</span> <span class="o">=</span>
    <span class="n">registerForActivityResult</span><span class="o">(</span>
        <span class="k">new</span> <span class="nc">ActivityResultContracts</span><span class="o">.</span><span class="na">StartActivityForResult</span><span class="o">(),</span>
        <span class="n">result</span> <span class="o">-&gt;</span> <span class="o">{</span>
          <span class="nc">Intent</span> <span class="n">resultIntent</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="na">getData</span><span class="o">();</span>
          <span class="k">if</span> <span class="o">(</span><span class="n">resultIntent</span> <span class="o">!=</span> <span class="kc">null</span> <span class="o">&amp;&amp;</span> <span class="n">result</span><span class="o">.</span><span class="na">getResultCode</span><span class="o">()</span> <span class="o">==</span> <span class="no">RESULT_OK</span><span class="o">)</span> <span class="o">{</span>
            <span class="nc">Bitmap</span> <span class="n">bitmap</span> <span class="o">=</span> <span class="kc">null</span><span class="o">;</span>
            <span class="k">try</span> <span class="o">{</span>
              <span class="n">bitmap</span> <span class="o">=</span>
                  <span class="nc">MediaStore</span><span class="o">.</span><span class="na">Images</span><span class="o">.</span><span class="na">Media</span><span class="o">.</span><span class="na">getBitmap</span><span class="o">(</span>
                      <span class="k">this</span><span class="o">.</span><span class="na">getContentResolver</span><span class="o">(),</span> <span class="n">resultIntent</span><span class="o">.</span><span class="na">getData</span><span class="o">());</span>
              <span class="c1">// Please also rotate the Bitmap based on its orientation.</span>
            <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="nc">IOException</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
              <span class="nc">Log</span><span class="o">.</span><span class="na">e</span><span class="o">(</span><span class="no">TAG</span><span class="o">,</span> <span class="s">"Bitmap reading error:"</span> <span class="o">+</span> <span class="n">e</span><span class="o">);</span>
            <span class="o">}</span>
            <span class="k">if</span> <span class="o">(</span><span class="n">bitmap</span> <span class="o">!=</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
              <span class="n">faceMesh</span><span class="o">.</span><span class="na">send</span><span class="o">(</span><span class="n">bitmap</span><span class="o">);</span>
            <span class="o">}</span>
          <span class="o">}</span>
        <span class="o">});</span>
<span class="nc">Intent</span> <span class="n">pickImageIntent</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Intent</span><span class="o">(</span><span class="nc">Intent</span><span class="o">.</span><span class="na">ACTION_PICK</span><span class="o">);</span>
<span class="n">pickImageIntent</span><span class="o">.</span><span class="na">setDataAndType</span><span class="o">(</span><span class="nc">MediaStore</span><span class="o">.</span><span class="na">Images</span><span class="o">.</span><span class="na">Media</span><span class="o">.</span><span class="na">INTERNAL_CONTENT_URI</span><span class="o">,</span> <span class="s">"image/*"</span><span class="o">);</span>
<span class="n">imageGetter</span><span class="o">.</span><span class="na">launch</span><span class="o">(</span><span class="n">pickImageIntent</span><span class="o">);</span>
</code></pre></div><button type="button" aria-label="Copy code to clipboard"><svg viewBox="0 0 24 24" class="copy-icon"><use xlink:href="#svg-copy"></use></svg></button></div> <h4 id="video-input"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#video-input" class="anchor-heading" aria-labelledby="video-input"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Video Input </h4> <div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// For video input and result rendering with OpenGL.</span>
<span class="nc">FaceMeshOptions</span> <span class="n">faceMeshOptions</span> <span class="o">=</span>
    <span class="nc">FaceMeshOptions</span><span class="o">.</span><span class="na">builder</span><span class="o">()</span>
        <span class="o">.</span><span class="na">setStaticImageMode</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
        <span class="o">.</span><span class="na">setRefineLandmarks</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
        <span class="o">.</span><span class="na">setMaxNumFaces</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
        <span class="o">.</span><span class="na">setRunOnGpu</span><span class="o">(</span><span class="kc">true</span><span class="o">).</span><span class="na">build</span><span class="o">();</span>
<span class="nc">FaceMesh</span> <span class="n">faceMesh</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">FaceMesh</span><span class="o">(</span><span class="k">this</span><span class="o">,</span> <span class="n">faceMeshOptions</span><span class="o">);</span>
<span class="n">faceMesh</span><span class="o">.</span><span class="na">setErrorListener</span><span class="o">(</span>
    <span class="o">(</span><span class="n">message</span><span class="o">,</span> <span class="n">e</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="nc">Log</span><span class="o">.</span><span class="na">e</span><span class="o">(</span><span class="no">TAG</span><span class="o">,</span> <span class="s">"MediaPipe Face Mesh error:"</span> <span class="o">+</span> <span class="n">message</span><span class="o">));</span>

<span class="c1">// Initializes a new VideoInput instance and connects it to MediaPipe Face Mesh Solution.</span>
<span class="nc">VideoInput</span> <span class="n">videoInput</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">VideoInput</span><span class="o">(</span><span class="k">this</span><span class="o">);</span>
<span class="n">videoInput</span><span class="o">.</span><span class="na">setNewFrameListener</span><span class="o">(</span>
    <span class="n">textureFrame</span> <span class="o">-&gt;</span> <span class="n">faceMesh</span><span class="o">.</span><span class="na">send</span><span class="o">(</span><span class="n">textureFrame</span><span class="o">));</span>

<span class="c1">// Initializes a new GlSurfaceView with a ResultGlRenderer&lt;FaceMeshResult&gt; instance</span>
<span class="c1">// that provides the interfaces to run user-defined OpenGL rendering code.</span>
<span class="c1">// See mediapipe/examples/android/solutions/facemesh/src/main/java/com/google/mediapipe/examples/facemesh/FaceMeshResultGlRenderer.java</span>
<span class="c1">// as an example.</span>
<span class="nc">SolutionGlSurfaceView</span><span class="o">&lt;</span><span class="nc">FaceMeshResult</span><span class="o">&gt;</span> <span class="n">glSurfaceView</span> <span class="o">=</span>
    <span class="k">new</span> <span class="nc">SolutionGlSurfaceView</span><span class="o">&lt;&gt;(</span>
        <span class="k">this</span><span class="o">,</span> <span class="n">faceMesh</span><span class="o">.</span><span class="na">getGlContext</span><span class="o">(),</span> <span class="n">faceMesh</span><span class="o">.</span><span class="na">getGlMajorVersion</span><span class="o">());</span>
<span class="n">glSurfaceView</span><span class="o">.</span><span class="na">setSolutionResultRenderer</span><span class="o">(</span><span class="k">new</span> <span class="nc">FaceMeshResultGlRenderer</span><span class="o">());</span>
<span class="n">glSurfaceView</span><span class="o">.</span><span class="na">setRenderInputImage</span><span class="o">(</span><span class="kc">true</span><span class="o">);</span>

<span class="n">faceMesh</span><span class="o">.</span><span class="na">setResultListener</span><span class="o">(</span>
    <span class="n">faceMeshResult</span> <span class="o">-&gt;</span> <span class="o">{</span>
      <span class="nc">NormalizedLandmark</span> <span class="n">noseLandmark</span> <span class="o">=</span>
          <span class="n">result</span><span class="o">.</span><span class="na">multiFaceLandmarks</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="na">getLandmarkList</span><span class="o">().</span><span class="na">get</span><span class="o">(</span><span class="mi">1</span><span class="o">);</span>
      <span class="nc">Log</span><span class="o">.</span><span class="na">i</span><span class="o">(</span>
          <span class="no">TAG</span><span class="o">,</span>
          <span class="nc">String</span><span class="o">.</span><span class="na">format</span><span class="o">(</span>
              <span class="s">"MediaPipe Face Mesh nose normalized coordinates (value range: [0, 1]): x=%f, y=%f"</span><span class="o">,</span>
              <span class="n">noseLandmark</span><span class="o">.</span><span class="na">getX</span><span class="o">(),</span> <span class="n">noseLandmark</span><span class="o">.</span><span class="na">getY</span><span class="o">()));</span>
      <span class="c1">// Request GL rendering.</span>
      <span class="n">glSurfaceView</span><span class="o">.</span><span class="na">setRenderData</span><span class="o">(</span><span class="n">faceMeshResult</span><span class="o">);</span>
      <span class="n">glSurfaceView</span><span class="o">.</span><span class="na">requestRender</span><span class="o">();</span>
    <span class="o">});</span>

<span class="nc">ActivityResultLauncher</span><span class="o">&lt;</span><span class="nc">Intent</span><span class="o">&gt;</span> <span class="n">videoGetter</span> <span class="o">=</span>
    <span class="n">registerForActivityResult</span><span class="o">(</span>
        <span class="k">new</span> <span class="nc">ActivityResultContracts</span><span class="o">.</span><span class="na">StartActivityForResult</span><span class="o">(),</span>
        <span class="n">result</span> <span class="o">-&gt;</span> <span class="o">{</span>
          <span class="nc">Intent</span> <span class="n">resultIntent</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="na">getData</span><span class="o">();</span>
          <span class="k">if</span> <span class="o">(</span><span class="n">resultIntent</span> <span class="o">!=</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
            <span class="k">if</span> <span class="o">(</span><span class="n">result</span><span class="o">.</span><span class="na">getResultCode</span><span class="o">()</span> <span class="o">==</span> <span class="no">RESULT_OK</span><span class="o">)</span> <span class="o">{</span>
              <span class="n">glSurfaceView</span><span class="o">.</span><span class="na">post</span><span class="o">(</span>
                  <span class="o">()</span> <span class="o">-&gt;</span>
                      <span class="n">videoInput</span><span class="o">.</span><span class="na">start</span><span class="o">(</span>
                          <span class="k">this</span><span class="o">,</span>
                          <span class="n">resultIntent</span><span class="o">.</span><span class="na">getData</span><span class="o">(),</span>
                          <span class="n">faceMesh</span><span class="o">.</span><span class="na">getGlContext</span><span class="o">(),</span>
                          <span class="n">glSurfaceView</span><span class="o">.</span><span class="na">getWidth</span><span class="o">(),</span>
                          <span class="n">glSurfaceView</span><span class="o">.</span><span class="na">getHeight</span><span class="o">()));</span>
            <span class="o">}</span>
          <span class="o">}</span>
        <span class="o">});</span>
<span class="nc">Intent</span> <span class="n">pickVideoIntent</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Intent</span><span class="o">(</span><span class="nc">Intent</span><span class="o">.</span><span class="na">ACTION_PICK</span><span class="o">);</span>
<span class="n">pickVideoIntent</span><span class="o">.</span><span class="na">setDataAndType</span><span class="o">(</span><span class="nc">MediaStore</span><span class="o">.</span><span class="na">Video</span><span class="o">.</span><span class="na">Media</span><span class="o">.</span><span class="na">INTERNAL_CONTENT_URI</span><span class="o">,</span> <span class="s">"video/*"</span><span class="o">);</span>
<span class="n">videoGetter</span><span class="o">.</span><span class="na">launch</span><span class="o">(</span><span class="n">pickVideoIntent</span><span class="o">);</span>
</code></pre></div><button type="button" aria-label="Copy code to clipboard"><svg viewBox="0 0 24 24" class="copy-icon"><use xlink:href="#svg-copy"></use></svg></button></div> <h2 id="example-apps"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#example-apps" class="anchor-heading" aria-labelledby="example-apps"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Example Apps </h2> <p>Please first see general instructions for <a href="https://google.github.io/mediapipe/getting_started/android.html">Android</a>, <a href="https://google.github.io/mediapipe/getting_started/ios.html">iOS</a> and <a href="https://google.github.io/mediapipe/getting_started/cpp.html">desktop</a> on how to build MediaPipe examples.</p> <p>Note: To visualize a graph, copy the graph and paste it into <a href="https://viz.mediapipe.dev/">MediaPipe Visualizer</a>. For more information on how to visualize its associated subgraphs, please see <a href="https://google.github.io/mediapipe/tools/visualizer.html">visualizer documentation</a>.</p> <h3 id="face-landmark-example"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#face-landmark-example" class="anchor-heading" aria-labelledby="face-landmark-example"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Face Landmark Example </h3> <p>Face landmark example showcases real-time, cross-platform face landmark detection. For visual reference, please refer to <em>Fig. 2</em>.</p> <h4 id="mobile"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#mobile" class="anchor-heading" aria-labelledby="mobile"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Mobile </h4> <ul> <li>Graph: <a href="https://github.com/google/mediapipe/tree/master/mediapipe/graphs/face_mesh/face_mesh_mobile.pbtxt"><code class="language-plaintext highlighter-rouge">mediapipe/graphs/face_mesh/face_mesh_mobile.pbtxt</code></a></li> <li>Android target: <a href="https://drive.google.com/open?id=1pUmd7CXCL_onYMbsZo5p91cH0oNnR4gi">(or download prebuilt ARM64 APK)</a> <a href="https://github.com/google/mediapipe/tree/master/mediapipe/examples/android/src/java/com/google/mediapipe/apps/facemeshgpu/BUILD"><code class="language-plaintext highlighter-rouge">mediapipe/examples/android/src/java/com/google/mediapipe/apps/facemeshgpu:facemeshgpu</code></a></li> <li>iOS target: <a href="http://mediapipe/examples/ios/facemeshgpu/BUILD"><code class="language-plaintext highlighter-rouge">mediapipe/examples/ios/facemeshgpu:FaceMeshGpuApp</code></a></li> </ul> <p>Tip: Maximum number of faces to detect/process is set to 1 by default. To change it, for Android modify <code class="language-plaintext highlighter-rouge">NUM_FACES</code> in <a href="https://github.com/google/mediapipe/tree/master/mediapipe/examples/android/src/java/com/google/mediapipe/apps/facemeshgpu/MainActivity.java">MainActivity.java</a>, and for iOS modify <code class="language-plaintext highlighter-rouge">kNumFaces</code> in <a href="https://github.com/google/mediapipe/tree/master/mediapipe/examples/ios/facemeshgpu/FaceMeshGpuViewController.mm">FaceMeshGpuViewController.mm</a>.</p> <h4 id="desktop"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#desktop" class="anchor-heading" aria-labelledby="desktop"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Desktop </h4> <ul> <li>Running on CPU <ul> <li>Graph: <a href="https://github.com/google/mediapipe/tree/master/mediapipe/graphs/face_mesh/face_mesh_desktop_live.pbtxt"><code class="language-plaintext highlighter-rouge">mediapipe/graphs/face_mesh/face_mesh_desktop_live.pbtxt</code></a></li> <li>Target: <a href="https://github.com/google/mediapipe/tree/master/mediapipe/examples/desktop/face_mesh/BUILD"><code class="language-plaintext highlighter-rouge">mediapipe/examples/desktop/face_mesh:face_mesh_cpu</code></a></li> </ul> </li> <li>Running on GPU <ul> <li>Graph: <a href="https://github.com/google/mediapipe/tree/master/mediapipe/graphs/face_mesh/face_mesh_desktop_live_gpu.pbtxt"><code class="language-plaintext highlighter-rouge">mediapipe/graphs/face_mesh/face_mesh_desktop_live_gpu.pbtxt</code></a></li> <li>Target: <a href="https://github.com/google/mediapipe/tree/master/mediapipe/examples/desktop/face_mesh/BUILD"><code class="language-plaintext highlighter-rouge">mediapipe/examples/desktop/face_mesh:face_mesh_gpu</code></a></li> </ul> </li> </ul> <p>Tip: Maximum number of faces to detect/process is set to 1 by default. To change it, in the graph file modify the option of <code class="language-plaintext highlighter-rouge">ConstantSidePacketCalculator</code>.</p> <h3 id="face-effect-example"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#face-effect-example" class="anchor-heading" aria-labelledby="face-effect-example"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Face Effect Example </h3> <p>Face effect example showcases real-time mobile face effect application use case for the Face Mesh solution. To enable a better user experience, this example only works for a single face. For visual reference, please refer to <em>Fig. 4</em>.</p> <h4 id="mobile-1"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#mobile-1" class="anchor-heading" aria-labelledby="mobile-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Mobile </h4> <ul> <li>Graph: <a href="https://github.com/google/mediapipe/tree/master/mediapipe/graphs/face_effect/face_effect_gpu.pbtxt"><code class="language-plaintext highlighter-rouge">mediapipe/graphs/face_effect/face_effect_gpu.pbtxt</code></a></li> <li>Android target: <a href="https://drive.google.com/file/d/1ccnaDnffEuIXriBZr2SK_Eu4FpO7K44s">(or download prebuilt ARM64 APK)</a> <a href="https://github.com/google/mediapipe/tree/master/mediapipe/examples/android/src/java/com/google/mediapipe/apps/faceeffect/BUILD"><code class="language-plaintext highlighter-rouge">mediapipe/examples/android/src/java/com/google/mediapipe/apps/faceeffect</code></a></li> <li>iOS target: <a href="http://mediapipe/examples/ios/faceeffect/BUILD"><code class="language-plaintext highlighter-rouge">mediapipe/examples/ios/faceeffect</code></a></li> </ul> <h2 id="resources"> <a href="https://google.github.io/mediapipe/solutions/face_mesh#resources" class="anchor-heading" aria-labelledby="resources"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Resources </h2> <ul> <li>Google AI Blog: <a href="https://ai.googleblog.com/2019/03/real-time-ar-self-expression-with.html">Real-Time AR Self-Expression with Machine Learning</a></li> <li>TensorFlow Blog: <a href="https://blog.tensorflow.org/2020/03/face-and-hand-tracking-in-browser-with-mediapipe-and-tensorflowjs.html">Face and hand tracking in the browser with MediaPipe and TensorFlow.js</a></li> <li>Google Developers Blog: <a href="https://developers.googleblog.com/2020/09/mediapipe-3d-face-transform.html">MediaPipe 3D Face Transform</a></li> <li>Paper: <a href="https://arxiv.org/abs/1907.06724">Real-time Facial Surface Geometry from Monocular Video on Mobile GPUs</a> (<a href="https://docs.google.com/presentation/d/1-LWwOMO9TzEVdrZ1CS1ndJzciRHfYDJfbSxH_ke_JRg/present?slide=id.g5986dd4b4c_4_212">poster</a>)</li> <li>Canonical face model: <a href="https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_geometry/data/canonical_face_model.fbx">FBX</a>, <a href="https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_geometry/data/canonical_face_model.obj">OBJ</a>, <a href="https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png">UV visualization</a></li> <li><a href="https://google.github.io/mediapipe/solutions/models.html#face_mesh">Models and model cards</a></li> <li><a href="https://code.mediapipe.dev/codepen/face_mesh">Web demo</a></li> <li><a href="https://mediapipe.page.link/face_mesh_py_colab">Python Colab</a></li> </ul> <hr> <footer> <p class="text-small text-grey-dk-100 mb-0">© GOOGLE LLC | <a href="https://policies.google.com/privacy">PRIVACY POLICY</a> | <a href="https://policies.google.com/terms">TERMS OF SERVICE</a></p> </footer> </div> </div> <div class="search-overlay"></div> </div>  
<div id="ef1f3574-66b8-42fc-9392-c21dc20b38db-quickmenu"></div></body></html>